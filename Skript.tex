\documentclass[a4paper]{article}

\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz,tkz-euclide}
\usepackage{titlesec}
\usepackage{gensymb}
\usepackage{textcomp}
\usepackage[titles]{tocloft}
\usepackage{csquotes}
\usepackage[babel]{microtype}
\usepackage{MnSymbol}
\usepackage{stmaryrd}
\usepackage{mathtools}
\usepackage{ulem}
\usepackage[
  separate-uncertainty = true,
  multi-part-units = repeat
]{siunitx}

\usetkzobj{all}
\usetikzlibrary{shapes.misc}

\MakeOuterQuote{"}

\newcommand*\circled[1]{
  \tikz[baseline=(C.base)]\node[draw,circle,inner sep=0.75pt](C) {#1};\!
}

\newcommand*{\obot}{\perp\mkern-20.7mu\bigcirc}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\renewcommand{\thesubsection}{\arabic{subsection}}
\titleformat{\section}{\normalfont\Large\bfseries}{Kapitel \arabic{section}: }{0em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{§\arabic{subsection} }{0em}{}
\titleformat{\subsubsection}{\normalfont\bfseries}{\arabic{subsection}.\arabic{subsubsection} }{0em}{}
\renewcommand{\cftsubsecpresnum}{§}
\newlength\mylength
\settowidth\mylength{\cftsubsecpresnum}
\settowidth\mylength{\cftsubsecaftersnum}
\addtolength\cftsubsecnumwidth{\mylength}
\renewcommand{\cftsecpresnum}{Kapitel }
\renewcommand{\cftsecaftersnum}{: }
\settowidth\mylength{\cftsecpresnum}
\addtolength\cftsecnumwidth{\mylength}

\newcommand{\ul}{\underline}
\renewcommand{\proof}{\ul{Beweis:}\\}
\renewcommand{\qed}{\begin{flushright}
\ul{\(q.e.d.\)}
\end{flushright}}
\let\origphi\phi
\let\phi\varphi
\let\origepsilon\epsilon
\let\epsilon\varepsilon

\title{Lineare Algebra II: Skript}
\author{Nico Mexis}
\date{\today}

\begin{document}
\maketitle
\newpage

\tableofcontents
\newpage

\setcounter{section}{4}
\section{Endomorphismen}
\setcounter{subsection}{17}
\subsection{Eigenwerte (Buch: §4.1-4.2)}
Sei \(K\) ein Körper.\\
Sei \(V\) ein endlich dimensionaler \(K\)-Vektorraum.\\
Sei \(\phi: V \rightarrow V\) ein Endomorphismus.
\subsubsection{Definition}
\begin{itemize}
\item[\circled{a}] Ein Element \(\lambda \in K\) heißt ein \ul{Eigenwert} von \(\phi\), wenn es einen Vektor \(v \in V\ \backslash \{0\}\) gibt mit \(\phi(v)=\lambda*v\).
\item[\circled{b}] Ist \(\lambda \in K\) ein Eigenwert von \(\phi\), so heißt jeder Vektor \(v \in V \backslash \{0\}\) mit \(\phi(v) = \lambda*v\) ein Eigenvektor von \(\phi\) zum Eigenwert \(\lambda\).
\item[\circled{c}] Ist \(A \in Mat_n(K)\), so heißt ein \(\lambda \in K\) ein \ul{Eigenwert} von A, wenn es ein \(v \in K^n\backslash\{0\}\) gibt mit \(A*v=\lambda*v\).
\end{itemize}
\subsubsection{Beispiel}
\begin{itemize}
\item[\circled{a}] Das Element \(0 \in K\) ist ein Eigenwert von \(\phi\), wenn \(\phi\) nicht injektiv ist.
\item[\circled{b}] Das Element \(1 \in K\) ist ein Eigenwert von \(\phi\), wenn \(\phi\) einen \ul{Fixpunkt} \(v \neq 0\) hat (d.h. \(\phi(v)=v\)).
\end{itemize}
\subsubsection{Beispiel}
Sei \(K = \mathbb{R}, V= \mathbb{R}^2\).\\
Sei \(\phi:\mathbb{R}^2\rightarrow\mathbb{R}^2\) die Drehung um \(0=(0,0)\) um den Winkel \(\alpha \in [0,2\pi[\).\\
\begin{itemize}
\item[\circled{a}] Ist \(\alpha = 0\), so ist \(\phi = id_{\mathbb{R}^2}\) und \(\lambda = 1\) ist der einzige Eigenwert von \(\phi\).
\item[\circled{b}] Ist \(\alpha = \pi\), so ist \(\phi = -id_{\mathbb{R}^2}\) und \(\lambda = -1\) ist der einzige Eigenwert von \(\phi\).
\item[\circled{c}] Ist \(\alpha \notin \{0,\pi\}\), so besitzt \(\phi\) keine Eigenwerte.
\end{itemize}
\subsubsection{Beispiel}
Sei \(K = \mathbb{R}, V= \mathbb{R}^2\) und \(\sigma :\mathbb{R}^2 \rightarrow\mathbb{R}^2\) die Spiegelung an der Geraden \(G\) durch \(0\), die mit der x-Achse einen Winkel \(\frac{\alpha}{2}\) einschließt.
Ein Eigenwert ist \(\lambda=1\) und die Menge der Eigenvektoren zum Eigenwert \(\lambda=1\) ist \(G\backslash\{0\}\).\\
Ein weiterer Eigenwert ist \(\lambda=-1\) und die Menge der Eigenvektoren zum Eigenwert \(\lambda=-1\) ist \(H\backslash\{0\}\), wobei \(H\) die zu \(G\) senkrechte Gerade durch 0 ist.
\subsubsection{Definition}
Sei \(\lambda \in K\) ein Eigenwert von \(\phi\). Dann heißt \(Eig(\phi,\lambda)=\{v \in V | \phi (v)=\lambda *v\}\) der \ul{Eigenraum} zum Eigenwert \(\phi\).
\subsubsection{Satz}
Sei \(\phi:V \rightarrow V\) ein Endomorphismus.
\begin{itemize}
\item[\circled{a}] Ist \(\lambda \in K\) ein Eigenwert von \(\phi\), so ist \(Eig(\phi,\lambda)\) ein Untervektorraum von V.
\item[\circled{b}] Sind \(\lambda,\mu \in K\) zwei verschiedene Eigenwerte von \(\phi\), so gilt: \(Eig(\phi,\mu)=\{0\}\).
\end{itemize}
\newpage
\ul{Beweis:}
\begin{itemize}
\item["\circled{a}"] Wegen \(\phi(0)=\lambda*0=0\) gilt \(\phi(v-w)=\phi(v)-\phi(w)=\lambda*v-\lambda*w=\lambda*(v-w)\), also \(v-w \in Eig(\phi,\lambda)\). Nach dem Untervektorraumkriterium folgt die Behauptung.
\item["\circled{b}"] Sei \(v \in Eig(\phi,\lambda)\cap Eig(\phi,\mu)\). Dann gilt \(\lambda*v=\phi(v)=\mu*v\), also \(\underbrace{(\lambda-\mu)}_{\neq 0}*v=0\). Dies liefert \(v=0\).
\end{itemize}
\qed
\subsubsection{Beispiel}
\begin{itemize}
\item[\circled{a}] Sei \(K = \mathbb{R}\), \(V = \mathbb{R}^2\) und \(\phi:\mathbb{R}^2\rightarrow \mathbb{R}^2\) die Spiegelung an der Winkelhalbierenden. Dann gilt \(M_{\xi}^{\xi}(\phi)=\begin{pmatrix}
0 & 1 \\
1 & 0
\end{pmatrix}\) und die Eigenwerte von \(\phi\) sind \(\lambda = \pm 1\)
\begin{itemize}
\item[\circled{1}] \(Eig(\phi,1)=\mathbb{R}*(1,1)\)
\item[\circled{2}] \(Eig(\phi,-1)=\mathbb{R}*(1,-1)\)
\end{itemize}
\item[\circled{b}] Sei \(K = \mathbb{R}\), \(V = \mathbb{R}^3\) und \(\phi:\mathbb{R}^3\rightarrow \mathbb{R}^3\) die Drehung um die z-Achse um einen Winkel \(\alpha \in ]0,\pi[\). Dann gilt \(M_{\xi}^{\xi}(\phi)=\begin{pmatrix}
cos(\alpha) & -sin(\alpha) & 0 \\
sin(\alpha) & cos(\alpha) & 0 \\
0 & 0 & 1
\end{pmatrix}\) und \(\lambda = 1\) ist der einzige Eigenwert mit \(Eig(\phi,1)=\mathbb{R}*\underbrace{(0,0,1)}_{z-Achse}\).
\end{itemize}
\subsubsection{Definition}
\begin{itemize}
\item[\circled{a}] Der Endomorphismus \(\phi:V\rightarrow V\) heißt \ul{diagonalisierbar}, wenn es eine Basis \(B\neq (v_1,\dots,v_d)\) von \(V\) gibt, die aus Eigenvektoren besteht.\\
Sei \(\phi(v_i)=a_iv_i\) mit \(a_i \in K\) für \(i=1,..,d\). Dann gilt also \(M^B_B(\phi)=\begin{pmatrix}
a_1 & 0 & \hdots & 0 \\
0 & a_2 & \hdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \hdots & a_n
\end{pmatrix}\) Diagonalmatrix.
\item[\circled{b}] Eine Matrix \(A \in Mat_n(K)\) heißt \ul{diagonalisierbar}, wenn der zugehörige Endomorphismus \(\phi_A:K^n\rightarrow K^n\) diagonalisierbar ist, d.h. wenn es ein \(T \in GL_n(K)\) gibt, sodass \(TAT^{-1}\) eine Diagonalmatrix ist.
\end{itemize}
\subsubsection{Satz (Charakterisierung der Diagonalisierbarkeit)}
Für einen Endomorphismus \(\phi:V\rightarrow V\) sind die folgenden Bedingungen äquivalent:
\begin{itemize}
\item[\circled{a}] \(\phi\) ist diagonalisierbar.
\item[\circled{b}] Es gibt eine Basis \(B\) von \(V\), sodass \(M_B^B(\phi)\) eine Diagonalmatrix ist.
\item[\circled{c}] Für jede Basis \(B\) von \(V\) ist \(M_B^B(\phi)\) eine diagonalisierbare Matrix.
\item[\circled{d}] Es gibt eine Basis \(B\) von \(V\), die aus Eigenvektoren von \(\phi\) besteht.
\end{itemize}
\ul{Beweis:}
\begin{itemize}
\item["\circled{a} \textrightarrow \space\circled{b}"] Def. 18.8a
\item["\circled{b} \textrightarrow \space\circled{c}"] Sei \(B\) eine Basis von \(V\), sodass \(M_B^B(\phi)\) eine Diagonalmatrix ist und sei \(C\) eine weitere Basis von \(V\). Dann gilt: \(M_B^B(\phi)=T_C^B*M_C^C(\phi)*(T_C^B)^{-1}\) mit \(T_C^B\in GL_n(K)\). Also ist \(M_C^C(\phi)\) diagonalisierbar.
\item["\circled{c} \textrightarrow \space\circled{d}"] Nach Def. 18.8b gibt es \(T \in GL_n(K)\) mit \(T*M_C^C(\phi)*T^{-1}\) Diagonalmatrix. Bzgl. der transponierten Basis \(B=T*C\) ist also \(M_B^B(\phi)\) eine Diagonalmatrix, d.h. \(B\) besteht aus Eigenvektoren von \(\phi\).
\item["\circled{d} \textrightarrow \space\circled{a}"] Nach Def.
\end{itemize}
\qed
\subsubsection{Beispiel}
Eigenwerte und Eigenvektoren spielen eine große Rolle beim Lösen von \ul{Differentialgleichungen}.\\
Suche z.B. im \(\mathcal{C}^1(\mathbb{R})\) die Lösungen von\\
\(f_1'(t)=f_1(t)-f_2(t)\)\\
\(f_2'(t)=2f_1(t)+4f_2(t)\).\\
Man macht folgenden Lösungsansatz: \(f_1(t)=c_1*e^{\lambda t}\) und \(f_2(t)=c_2*e^{\lambda t}\)\\
\(\Rightarrow \begin{pmatrix}
f_1'(t) \\
f_2'(t)
\end{pmatrix} = \begin{pmatrix}
\lambda c_1e^{\lambda t} \\
\lambda c_2e^{\lambda t}
\end{pmatrix} = \lambda * \begin{pmatrix}
f_1(t) \\
f_2(t)
\end{pmatrix}\).\\
Das Differentialgleichungssystem lautet also:\\
\(\begin{pmatrix}
f_1'(t) \\
f_2'(t)
\end{pmatrix} = \lambda * \begin{pmatrix}
f_1(t) \\
f_2(t)
\end{pmatrix} = \begin{pmatrix}
1 & -1 \\
2 & 4
\end{pmatrix} * \begin{pmatrix}
f_1(t) \\
f_2(t)
\end{pmatrix}\).\\
Im \(\mathbb{R}\)-Vektorraum \(\mathcal{C}^1(\mathbb{R})\) ist \(\begin{pmatrix}
f_1'(t) \\
f_2'(t)
\end{pmatrix}\) ein Eigenvektor der Matrix \(\begin{pmatrix}
1 & -1 \\
2 & 4
\end{pmatrix}\) zum Eigenwert \(\lambda\).
\subsubsection{Satz}
Ist \(\lambda \in K\) ein Eigenwert von \(\phi\), so gilt: \(Eig(\phi, \lambda)=Ker(\phi-\lambda*id_V)\).\\
\ul{Beweis:}\\
Es gilt \(v \in Eig(\phi, \lambda) \Leftrightarrow \phi(v)=\lambda *v \Leftrightarrow(\phi-\lambda*id_V)(v)=0 \Leftrightarrow v\in Ker(\phi-v*id_V)\).
\qed
\subsubsection{Definition}
Sei \(x\) eine Unbestimmte und \(\mathcal{A} = (a_{ij}) \in Mat_n(K)\). Dann gilt: \(\mathcal{A}-x*\mathcal{I}_n=\begin{pmatrix}
a_{11}-x & a_{12} & \hdots & a_{1n} \\
a_{21} & a_{22}-x & \hdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \hdots & a_{nn}-x
\end{pmatrix} \in Mat_n(K[x])\).\\
Dann heißt \(det(\mathcal{A}-x*\mathcal{I}_n)\in K[x]\) das \ul{charakteristische Polynom} von \(\mathcal{A}\) und wird mit \(\underset{"chi"}{\chi_A}(x)\) bezeichnet.
\subsubsection{Beispiel}
Sei \(\mathcal{A}=\begin{pmatrix}
0 & 1 \\
1 & 0
\end{pmatrix} \in Mat_2(\mathbb{Q})\). Dann gilt \(\chi_A(x)=det\begin{pmatrix}
-x & 1 \\
1 & -x
\end{pmatrix}=x^2-1\).
\subsubsection{Satz}
Seien \(B,C\) zwei Basen von \(V\). Dann gilt:\\
\(\chi_{M_B^B(\phi)}(x)=\chi_{M_C^C(\phi)}(x)\)\\
\ul{Beweis:}\\
Nach der Transformationsformel gilt:\\
\(M_C^C(\phi)=T_C^B*M_B^B(\phi)*T_B^C\)\\
Es folgt: \(\chi_{M_C^C(\phi)}(x)=det(T_C^BM_B^B(\phi)T_B^C-\underbrace{T_C^Bx\mathcal{I}_nT_B^C}_{=x\mathcal{I}_n})\\=det(T_C^B(M_B^B(\phi)-x\mathcal{I}_n)T_B^C)\\=det(T_C^B)*det(M_B^B(\phi)-x*\mathcal{I}_n)*det(T_C^B)^{-1}=\chi_{M_B^B(\phi)}(x)\).
\qed
\ul{Folgerung:} Das Polynom \(\chi_{M_B^B(\phi)}(x)\) hängt nur von \(\phi\), aber nicht von der Wahl der Basis \(B\) ab. Es heißt das \ul{charakteristische Polynom} von \(\phi\) und wird mit \(\chi_\phi(x)\) bezeichnet.
\subsubsection{Satz}
Ein Element \(\lambda \in K\) ist genau dann ein Eigenwert von \(\phi\), wenn \(\chi_{\phi}(\lambda)=0\) gilt.\\
\ul{Beweis:}\\
Genau dann ist \(\lambda \in K\) ein Eigenwert von \(\phi\), wenn \(Ker(\phi-\lambda id_V)\neq \{0\}\) gilt. Dies ist genau dann der Fall, wenn \(\phi-\lambda id_V\) nicht bijektiv ist. Letztere Bedingung ist äquivalent zu \(det(\phi-\lambda id_V)=0\), also zu \(\chi_\phi(\lambda)=0\).
\qed
\subsubsection{Beispiel}
\begin{itemize}
\item[\circled{a}] Sei \(\phi:\mathbb{R}^2\rightarrow\mathbb{R}^2\) die Spiegelung an der Winkelhalbierenden \(G=\mathbb{R}*\begin{pmatrix}
1 \\
1
\end{pmatrix}\). Dann gilt: \(M_\xi^\xi(\phi)=\begin{pmatrix}
0 & 1 \\
1 & 0
\end{pmatrix}\) und  \(\chi_\phi(x)=x^2-1=(x-1)(x+1)\). Also besitzt \(\phi\) zwei Eigenwerte, nämlich \(\lambda_1=1\) und \(\lambda_2=-1\).\\
Für \(\lambda_1=1\) gilt \(Eig(\phi,\lambda_1)=Ker(\phi-\lambda_1 id_{\mathbb{R}^2})=Ker\begin{pmatrix}
-1 & 1 \\
1 & -1
\end{pmatrix} = \\<\begin{pmatrix}
1 \\
1
\end{pmatrix}> = G\).\\
Für \(\lambda_2=-1\) gilt \(Eig(\phi,\lambda_2)=Ker(\phi-\lambda_2 id_{\mathbb{R}^2})=Ker\begin{pmatrix}
1 & 1 \\
1 & 1
\end{pmatrix} = \\<\begin{pmatrix}
-1 \\
1
\end{pmatrix}> = G^\bot\).
\item[\circled{b}] Sei \(\psi:\mathbb{R}^2\rightarrow\mathbb{R}^2\) die Drehung um \(0\) um den Winkel \(\alpha \in ]0,\pi[\). Dann gilt \(M_\xi^\xi(\psi)=\begin{pmatrix}
cos(\alpha) & -sin(\alpha) \\
sin(\alpha) & cos(\alpha)
\end{pmatrix}\).\\
\(\chi_\psi(x)=det\begin{pmatrix}
cos(\alpha)-x & -sin(\alpha) \\
sin(\alpha) & cos(\alpha)-x
\end{pmatrix}=(cos(\alpha)-x)^2+sin^2(\alpha)=x^2-2cos(\alpha)x+sin^2(\alpha)+cos^2(\alpha)=x^2-2cos(\alpha)x+1\).\\
Dieses quadratische Polynom hat die Diskriminante \(\triangle=4cos^2(\alpha)-4=-4sin^2(\alpha)<0\). Somit besitzt \(\psi\) keine Eigenwerte.
\end{itemize}
\subsubsection{Bemerkung}
Ein Polynom \(f \in K[x]\backslash K\) braucht keine Nullstelle in \(K\) zu besitzen (z.B. \(x^2+1 \in \mathbb{R}[x])\).\\
Der \ul{Fundamentalsatz der Algebra} besagt, dass jedes Polynom \(f \in \mathbb{C} [x] \backslash \mathbb{C}\) eine Nullstelle besitzt und somit in ein Produkt von Linearfaktoren zerfällt.
\subsubsection{Satz (Eigenschaften des charakteristischen Polynoms)}
Sei \(\phi:V\rightarrow V\ K\)-linear und \(n=dim_K(V)\). Dann gilt:
\begin{itemize}
\item[\circled{a}] \(deg(\chi_\phi(x))=n\).
\item[\circled{b}] Der Gradkoeffizient von \(\chi_\phi(x)\) ist \((-1)^n\), also \(\chi_\phi(x)=(-1)^nx^n+(Monome\ kleineren\ Grades)\)
\item[\circled{c}] Der konstante Term von \(\chi_\phi(x)\) ist \(det(\phi)=\chi_\phi(0)\).
\end{itemize}
\ul{Beweis:}\\
\begin{itemize}
\item["\circled{a},\circled{b}"] Sei \(\mathcal{A}=(a_{ij})\in Mat_n(K)\) eine Darstellungsmatrix von \(\phi\). Dann gilt \(\chi_\phi(x)=det\begin{pmatrix}
a_{11}-x & & * \\
 & \ddots & \\
 * & & a_{nn}-x
\end{pmatrix}=(a_{11}-x)(a_{22}-x)*\dots*(a_{nn}-x)+(Monome\ niedriegeren\ Grades\ als\ n)=(-x)^n+(Monome\ niedriegeren\ Grades\ als\ n)\)
\item["\circled{c}"] \(\chi_\phi(0)=det(\phi-0*id_V)=det(\phi)\).
\end{itemize}
\qed
\subsubsection{Bemerkung (Der zweite Koeffizient von \(\chi_\phi(x)\))}
Schreibe \(\chi_\phi(x)=(-1)^nx^n+c_{n-1}x^{n-1}+\dots+c_1x+c_0\). Dann heißt \(Spur(\phi)=\underset{"Trace"}{Tr}(\phi)=(-1)^{n-1}c_{n-1}\) die \ul{Spur} von \(\phi\).\\
Sie hängt nicht ab von der Wahl einer Darstellungsmatrix \(\mathcal{A}=(a_{ij})\). Es gilt \(Spur(\mathcal{A})=a_{11}+a_{22}+\dots+a_{nn}\).\\
\ul{Beweis:}\\
Um einen Summanden \(c*x^{n-1}\) in \(det\begin{pmatrix}
a_{11}-x & & & * \\
 & a_{22}-x & & \\
 & & \ddots & \\
* & & & a_{nn}-x
\end{pmatrix}\) zu erhalten, muss man genau \(n-1\)mal den Faktor \(a_{ii}-x\) wählen. Also suchen wir den Koeffizienten von \(x^{n-1}\) in \((a_{11}-x)*\dots*(a_{nn}-x)\). Beim Ausmultiplizieren muss man \((n-1)\)mal \(-x\) wählen und einmal \(a_{ii}\). Also gilt \(c=(-1)^{n-1}(a_{11}+a_{22}+\dots+a_{nn})=(-1)^{n-1} Spur(\phi)\).
\qed
HIER FEHLT VIEL
\setcounter{subsection}{21}
\subsection{Verallgemeinerte Eigenräume}
Sei \(K\) Körper, \(V\) endlich-dimensionaler \(K\)-Vektorraum, \(\phi \in End_K(V)\)\\
\(\chi_\phi(v)=det(\phi-x\cdot id_V)\) charakteristisches Polynom\\
\(\mu_\phi(x)=\) normiertes Polynom kleinsten Grades mit \(\mu_\phi (\phi) = 0 \hat{=}\) Minimalpolynom von \(\phi\).
\subsubsection{Definition (Großer Kern und kleines Bild)}
(a) Die Kette \(Ker(\phi) \subseteq Ker(\phi^2) \subseteq Ker(\phi^3) \subseteq \dots\) von \(K\)-Untervektorräumen von \(V\) wird stationär. Der Untervektorraum \(BigKer(\phi)=Ker(\phi^i)\) mit \(i >> 0\) heißt der \ul{große Kern} von \(\phi\).
(b) Die Kette \(Im(\phi) \supseteq Im(\phi^2) \supseteq \dots\) wird stationär. Der Untervektorraum \(SmIm(\phi) = Im(\phi^i)\) mit \(i >> 0\) heißt das \ul{kleine Bild} von \(\phi\).
\subsubsection{Satz (Das Lemma von Fitting)}
(a) Gilt \(Ker(\phi^m)=Ker(\phi^{m+1})\) für ein \(m \geq 1\), so gilt \(Ker(\phi^m)=Ker(\phi^i)\) für alle \(i \geq m\) und somit \(BigKer(\phi)=Ker(\phi^m)\).
(b) Gilt \(Im(\phi^{m'})=Im(\phi^{m'+1})\) für ein \(m' \geq 1\), so folgt \(Im(\phi^{m'})=Im(\phi^i)\) für alle \(i \geq m'\) und somit \(SmIm(\phi)=Im(\phi^{m'})\).
(c) Sind \(m,m'\) in (a) und (b) minimal, so folgt \(m=m'\).
(d) Die Ketten \({0} \subseteq Ker(\phi) \subsetneq Ker(\phi^2) \subsetneq \dots \subsetneq Ker(\phi^m)=BigKer(\phi)\) und \(V \supseteq Im(\phi) \supsetneq Im(\phi^2) \supsetneq \dots \supsetneq Im(\phi^{m'}) = SmIm(\phi)\) enthalten strikte Inklusionen.
(e) \fbox{\(V=BigKer(\phi) \oplus SmIm(\phi)\)}
\ul{Beweis:}\\
"a" Es genügt \(Ker(\phi^{m+1}) =Ker(\phi^{m+2})\) zu beweisen. Zeige "\(\supseteq\)".Sei \(v \in Ker(\phi^{m+2})\), also \(\phi^{m+1}(v)=0\), so folgt \(\phi(v)\in Ker(\phi^{m+1})=Ker(\phi^m)\), also \(\phi^{m+1}(v)= \phi^m(\phi(v))=0\) und somit \(v \in Ker(\phi^{m+1})\).
"b" Zu zeigen ist nur \(Im(\phi^{m+1}) \subseteq Im(\phi^{m+2})\). Dazu sei \(v\in Im(\phi^{m+1})\), also \(v=\phi^{m+1}(w)\) mit \(w\in V\). Dann folgt \(v=\phi(\overset{~}{v})\) mit \(\overset{~}{v} = \phi^m(w)\in I(\phi^m)=Im(\phi^{m+1})\). Somit gilt \(v~ = \phi^{m+1}(w~)\) mit \(w~ \in V\) und \(v=\phi(v~)=\phi^{m+1}(w~)\in Im(\phi^{m+1})\).
"c" Es gilt \(dim_K(V)=dim_K(Ker(\phi^m))+dim_K(Im(\phi^m))\) und \(dim_K(V)=dim_K(Ker(\phi^{m+1})+dim_K(Im(\phi^{m+1})\).
Aus \(Ker(\phi^m)=Ker(\phi^{m+1})\) folgt damit \(dim_K(Im(\phi^m))=dim_K(\phi^{m+1})\) und daher \(Im(\phi^m)=Im(\phi1{m+1})\).
"d" folgt aus (a) und (b).
"e" Nach (c) gilt \(dim_K(V)=dim_K(Ker(\phi^m))+dim_K(Im(\phi^m))\) für das minimale \(m\). (Insbesondere gilt: \(BigKer(\phi)=Ker(\phi^m)\) und \(SmIm(\phi)=Im(\phi^m)\) für dieses \(m\)).\\
Zu zeigen ist also \(Ker(\phi^m) \cap Im(\phi^m)={0}\).\\
Sei \(v = \phi^m(w)\in Ker(\phi^m)\) für ein \(w\in V\). Dann gilt \(\phi^m(v)=\phi^{2m}(w)=0\), also \(w\in Ker(\phi^{2m})=Ker(\phi^m)\) und somit \(v=\phi^m(w)=0\).
\qed
\ul{Wdh.:}\\
Sei \(\lambda\in K\) ein Eigenwert von \(\phi\). Dann ist \(x-\lambda\) ein Eigenfaktor und \(Eig(\phi, \lambda)=Ker(\phi-\lambda\cdot id_V)={v\in V | \phi(v)=\lambda v}\).
\subsubsection{Definition}
Sei \(\phi\in End_K(V)\) und \(\mu_\phi(x)^m_1\dots p_s(x)^{m_s}\) mit den Eigenfaktoren \(p_1(x),\dots,p_s(x)\) und mit \(m_i\geq 1\).
(a) der \(K\)-Untervektorraum \(Eig(\phi, p_i(x)) = Ker(p_i(\phi))\) heißt der \ul{Eigenraum} von \(\phi\) bzgl. des Eigenfaktors \(p_i(x)\):
(b) Der \(K\)-Untervektorraum \(Gen(\phi, p_i(x))=BigKer(p_i(\phi))\) heißt der \ul{verallgemeinerte  Eigenraum} (oder \ul{Hauptraum}) von \(\phi\) bzgl. \(p_i(x)\).
\subsubsection{Bemerkung}
Ist \(p_i(x)=x-\lambda\) mit \(\lambda\in K\) Eigenwert, so gilt:
(a) \(Eig(\phi,p_i(x))=Ker(\phi-id_V)=Eig(\phi,\lambda)\).
(b) \(Gen(\phi,p_i(x))=BigKer(\phi-\lambda\cdot id_V)=Ker(\phi-\lambda\cdot id_V)^m=\underset{Hau(\phi, \lambda)}{Gen(\phi, \lambda)}\). TODO PFEIL m>>0 BEI m
\subsubsection{Lemma}
Sei \(p(x)\in K[x]\backslash\{0\}\) ein Vielfaches von \(\mu_\phi(x)\). Es gelte \(p(x)=q_1(x)q_2(x)\) mit \(ggT(q_1(x),q_2(x))=1\). Ferner sei \(W_1=Ker(q_1(\phi))\) und \(W_2=Ker(q_2(\phi))\). Dann gilt:
(a) Für jedes \(f\in K[x]\) sind \(W_1\) und \(W_2 f(\phi)\)-invariant.
(b) Für \(i \neq j\) ist \(q_i(\phi)\Big|_{W_j}: W_j \rightarrow W_j\) ein Isomorphismus. TODO Einschränkung
(c) \(V=W_1 \oplus W_2\)
(d) Ist \(p(x)=\mu_\phi(x)\), so gilt \(q_i(x)=\mu_{\phi_{|_{W_i}}}(x)\) für \(i=1,2\). TODO Einschränkung
\ul{Beweis:}
"a" Da \(f(\phi),q_1(\phi),q_2(\phi)\) kommentieren, gilt für \(v\in W_i=Ker(q_i(\phi))\) dass \(q_i(\phi)\underbrace{f(\phi)(v)}_{z.z.\in W_i}=f(\phi)q_i(\phi)(v)=f(\phi)(0)=0\) und somit \(f(\phi)(v)\in Ker(q_i(\phi))=W_i\).\\
"b" Wegen \(ggT(q_1(x),q_2(x))=1\) gibt es \(f_1(x),f_2(x)\in K[x]\) mit \(f_1(x)\cdot q_1(x)+f_2(x)\cdot q_2(x)=1\). Für \(v\in V\) folgt damit \(f_1(\phi)q_1(\phi)(v)+f_2(\phi)q_2(\phi)(v)=v\) (*). TODO EINRAHMEN\\
Hieraus folgt \(W_1 \cap W_2 = Ker(q_1(\phi)) \cap Ker(q_2(\phi))={0}\) nach (*) und daher ist \(q_i(\phi)|W_j:W_j\rightarrow W_j\) injektiv, also bijektiv. TODO Einschränkung
"c" Es ist nur noch zu zeigen, dass \(V=W_1+W_2\) gilt. Sei \(v\in V\). Schreibe \(v=w_1+w_2\) mit \(w_i=f_i(\phi)q_i(\phi)(v)\) nach (*).\\
Dann folgt für \(j \neq i q_j(\phi)(w_i)=q_j(\phi)f_i(\phi)q_i(\phi)(v)=f_i(\phi)\underbrace{p(\phi)}_{=0}(v)=0\) und somit \(w_i\in Ker(q_j(\phi))=W_j\). Dies zeigt \(v\in W_1+W_2\).
"d" Sei \(q_i'(x)\) das Minimalpolynom von \(\phi\Big|_{W_i}\).
Nach Def. von \(W_j\) gilt \(q_i'(x)\Big|_{q_i(x)}\), denn \(q_i(\phi\Big|_{W_i})=q_i(\phi)\Big|_{W_i}=0\).
Also sind auch \(q_1'(x)\) teilerfremd und \(q_1'(x)q_2'(x)=\mu_\phi(x)=q_1(x)q_2(x)\), also \(q_1'(x)=q_1(x)\) und \(q_2'(x)=q_2(x)\). TODO Einschränkung
\qed
\subsubsection{Definition}
Ein Endomorphismus \(\phi\in End_K(V)\) heißt \ul{nilpotent}, wen es ein \(m\geq 1\) gibt mit \(\phi^m=0\). In diesem Fall heißt das kleinste solche m der \ul{Nilpotenzindex} von \(\phi\) und wird mit \(nix(\phi)\).
\subsubsection{Beispiel}
Sei \(\phi\in End_\mathbb{Q}(\mathbb{Q}^3)\) mit \(M_\xi^\xi(\phi)= TODO MATRIX\). Dann gilt \(M_\xi^\xi(\phi^2)=TODO MATRIX\) und \(\phi^3=0\), also \(nix(\phi)=3\).
\subsubsection{Lemma}
Seien \(U', U''\) zwei \(\phi\)-invariante \(K\)-Untervektorräume von \(V\) mit \(V=U' \oplus U''\).
Sei \(\phi'=\mu_{\phi \Big|_{U'}}\) und \(\phi''=\mu_{\phi \Big|_{U''}}\). Dann gilt \(\mu_\phi(x)=kgV(\mu_{\phi \Big|_{U'}}(x),\mu_{\phi \Big|_{U''}}(x))\).\\
\ul{Beweis:}\\
Es gilt \(\mu_\phi(\phi)=0\). Also folgt \(\mu_\phi(\phi\Big|_{U'})=0\) und \(\mu_\phi(\phi \Big|_{U''})=0\). Also ist \(\mu_\phi\) ein gemeinsames Vielfaches von \(\mu_{\phi'}\) und \(\mu_{\phi''}\). Umgekehrt sei \(f(x)=kgV(\mu_{\phi'},\mu_{\phi''}(x))\). Zu zeigen: \(f(\phi)=0\). Betrachte \(f(\phi)\backslash U'=f(\phi \Big|_{U'})=0\) da \(f\) ein Vielfaches von \(\mu_{\phi'}\) ist. Genauso gilt \(f(\phi) \Big|_{U''}=f(\phi|U'')=0\). Wegen \(V=U' \oplus U''\) folgt \(f(\phi)=0\), also \(\mu_\phi \Big|_{f}\).
\qed
\subsubsection{Theorem (Die verallgemeinerte Eigenraumzerlegung / Die Hauptraumzerlegung / Die Primärzerlegung von \(\phi\))}
Sei \(\phi\in End_K(V)\) und \(\mu_\phi(x)=p_1(x)^{m_1}\dots p_s(x)^{m_s}\) die Zerlegung in Eigenfaktoren.
(a) \(V=Gen(\phi,p_1(x)) \oplus \dots \oplus Gen(\phi, p_s(x))\).
(b) Die Einschränkung von \(p_i(\phi)\) auf \(\underbrace{Gen(\phi,p_i(x))}_{G_i}\) ist nilpotent mit \(nix(p_i(\phi)\Big|_{G_i})=m_i\).
(c) Die Einschränkung von \(p_i(\phi)\) auf \(G_j\) mit \(j \neq i\) sind Isomorphismen.
(d) Es gibt \(G_i=Gen(\phi,p_i(x))=Ker(p_i(\phi)^{m_i})=BigKer(p_i(\phi))\)
\ul{Beweis:}\\
Wir schließen mit vollständiger Induktion nach \(s\).\\
\ul{\(s=1\)}: Sei \(\mu_\phi(x)=p_1(x)^{m_1}\). Dies bedeutet \(p_1(\phi)^{m_1}=0\), d.h. \(p_1(\phi)\) ist nilpotent und \(nix(p_1(\phi))=m_1\) nach Def. von \(\mu_\phi(x)\). Offenbar folgt \(V=Ker(p_1(\phi)^{m_1})=BigKer/p_1(\phi))\).\\
\ul{\(s > 1\)}: Verwende das Lemma mit \(q_1(x)=p_1(x)^{m_1}\dots p_{s-1}(x)^{m_{s-1}}\) und \(q_2(x)=p_s(x)^{m_s}\). Nach Teil (c) des Lemmas folgt \(V=Ker(q_1(\phi)) \oplus Ker(q_2(\phi))\underset{I.V.}{=}(Ker(p_1(\phi))^{m_1} \oplus \dots \oplus Ker(p_{s-1}(\phi)^{m_{s-1}}) \oplus Ker(p_s(\phi)^{m_s})\). Nach Teil (d) des Lemmas ist jeweils \(p_i(x)^{m_i}\) das Minimalpolynom von \(p_i|G_i\), Dies beweist (b). Teil (c) folgt aus Teil (b) des Lemmas. Teil (d) folgt aus (b).
TODO UNDERSET
\qed
\subsubsection{Beispiel}
Sei \(K = \mathbb{Q}\) und \(\phi \in End_\mathbb{Q}(\mathbb{Q}^3)\) mit Matrix \(M_\chi^\chi(\phi)=\begin{pmatrix}
e4 & e5 & e6 & e7 & e3 & e7 & e8 & e3
\end{pmatrix}\).\\
Dann \(\mu_\phi(x)=x^6-x^2=x^2(x-1)(x+1)(x^2+1)\)\\
\(\xi_\phi(x)=x^4(x-1)(x+1)(x^2+1)\)\\
Nun berechnen wir: \(Gen(\phi, x+1)=Ker(\phi+id_V)=<(0,0,1,0,0,-1,1,-1)>\)\\
\(Gen(\phi, x-1)=Ker(\phi-id_V)=<(0,0,-1,0,0,-1,-1,-1)>\)\\
\(Gen(\phi, x^2+1)=Ker(\phi^2+id_V)=<(0,0,1,0,0,0,-1,0)<(0,0,0,0,0,1,0,-1)>\)\\
\(Gen(\phi, x)=BigKer(\phi)=Ker(\phi^2)=<(0,0,0,1,0,-1,0,0),(0,0,0,0,1,0,0,-1),(1,0,-1,0,0,0,0,0),(0,1,0,0,0,0,-1,0)>\)\\
Insgesamt folgt \(V=Ker(\phi+id_V)\oplus Ker(\phi-id_V) \oplus Ker(\phi^2+id_V)\oplus Ker(\phi^2)\).\\
Beachte: \(\underbrace{Ker(\phi)}_{=<e_4-e_6,e_5-e_8>} \subsetneq Ker(\phi^2)\).
\subsection{Nilpotente Endomorphismen (Buch: §4.5-4.6}
Sei \(K\) ein Körper, \(V\) ein endlich-dimensionaler \(K\)-Vektorraum und \(\phi\in End_K(V)\).
\(\phi\) nilpotent \(\Leftrightarrow\) es gibt ein \(m\geq 1\) mit \(\phi^m=0\). Das minimale solche \(m\) heißt der Nilpotenzindex \(nix(\phi)\).\\
Im Folgenden sei \(\phi\) nilpotent und \(m=nix(\phi)\).
\subsubsection{Definition}
(a) Die Kette \({0} \subsetneq Ker(\phi) \subsetneq Ker(\phi^2) \subsetneq \dots \subsetneq Ker(\phi^m)=V\) heißt die \ul{kanonische Filtrierung} von \(V\) bzgl. \(\phi\).
(b) Für \(i=1,\dots,m\) sei \(\delta_i=dim_K Ker(\phi^i)-dim_K(Ker(\phi^{i-1})\).
\subsubsection{Lemma}
Für \(i \geq 1\) gilt: \(\phi(Ker(\phi^i))\subseteq Ker(\phi^{i-1})\)\\
\proof
Dies folgt aus \(v\in Ker(\phi^i)\Rightarrow \phi^{i-1}(\phi(v))=0\Rightarrow \phi(v)\in Ker(\phi^{i-1})\).
\qed
\subsubsection{Bemerkung}
Sei \(\phi\) nilpotent mit \(m=nix(\phi)=2\).\\
Dann gilt: \(\{0\}\subsetneq Ker(\phi)\subsetneq Ker(\phi^2) =V\). Wähle ein Komplement \(U_2\) und \(Ker(\phi)\) in \(V\) und erhalte: \(V = Ker(\phi)\oplus U_2\).
Nach dem Lemma gilt: \(\phi(Ker(\phi^1))\subseteq Ker(\phi^0)=\{0\}\) und \(\phi(U_2)=\phi(V)=\phi(Ker(\phi^2))\subseteq Ker(\phi)\).\\
Wähle nun ein Komplement \(U_1\) von \(\phi(U_2)\) in \(Ker(\phi)\) und erhalte: \(v=Ker(\phi)\oplus U_2=U_1\oplus U_2\oplus \phi(U_2)\).\\
Wegen \(U_2\cap Ker(\phi)=\{0\}\) gilt dabei \(\phi EINGESCHR_{U_2}\) ist injektiv und somit \(dim_K(\phi(U-_2))=dim_K(U_2)\).\\
Also folgt: \(\delta_1=dim_K(Ker(\phi))=dim_K(U_1)+dim_K(\phi(U_2))\)\\
\(\delta_2=dim_K(V)-dim_K(Ker(\phi))=dim_K(U_2)\).
\subsubsection{Satz (Die Jordan-Zerlegung nilpotenter Endomorphismen)}
Sei \(\phi\in End_K(V)\) nilpotent und \(s=nix(\phi)\). Für\( i=1,\dots,s\) sei \(\delta_i=dim_K(Ker(\phi^i))-dim_K(Ker(\phi^{i-1}))\).\\
Konstruiere Untervektorräume \(U_1,\dots U_s\) von \(V\) rekursiv wie folgt:\\
(1) Sei \(U_s\) ein Komplement von \(Ker(\phi^{s-1})\) in \(V\).
(2) Absteigend für \(i=s-1, s-2,\dots, 1\) sei \(U_i\) ein Komplement von \(Ker(\phi^{i-1})+\phi^{s-i}(U_s)+\phi^{s-i-1}(U_{s-1})+\dots+\phi^1(U_{i+1})\) in \(Ker(\phi^i)\).\\
Dann gilt: (a) \(V=U_1\)\\
\(\oplus U_2\oplus \phi(U_2)\)\\
\(\oplus U_3\oplus \phi(U_3)\oplus \phi^2(U_3)\)\\
\vdots\\
\(\oplus U_s\oplus \phi(U_s)\oplus\dots\oplus\phi^{s-1}(U_s)\)\\
Diese Darstellung heißt die \ul{Jordan-Zerlegung} von \(V\) bzgl. \(\phi\).
(b) Für \(i=1,\dots,s\) gilt: \(\delta_i=\sum_{j=i}^{s} dim_K(U_j)\) und \(\delta_1\geq\delta_2\geq\dots\geq\delta_s>0\).\\
\ul{Beweis:} Vgl. Buch S.265-267.
\subsubsection{Korollar}
(a) Gilt \(\delta_1=\dots=\delta_s=1\), so folgt \(U_1=\dots=U_{s-1}=\{0\}\) und \(V=U_s\oplus \phi(U_s)\oplus\phi^2(U_s)\oplus\dots\oplus\phi^{s-1}(U_s)\).\\
Dabei gilt: \(Ker(\phi^i)=\phi^{s-i}(U_s)\oplus\dots\oplus\phi^{s-1}(U_s)\).
(b) Gilt \(\delta_1=1\), so folgt \(\delta_2=\dots=\delta_s=1\). In diesem Fall liefert jeder Vektor \(v\in V\backslash Ker(\phi^{s-1})\) ein Basistupel \(B=(v,\phi(v),\phi^2(v),\dots,\phi^{s-1}(v))\).\\
In dieser Basis gilt \(M_B^B(\phi)=\begin{pmatrix}
0 & 0 & & & 0\\
1 & 0 & \mbox{\Huge 0} & & \vdots\\
0 & 1 & & & \vdots\\
\vdots & \vdots & \ddots & & 0\\
0 & 0 & \hdots & 1 & 0
\end{pmatrix}\). TODO MATRIX SCHAUEN OB RICHTIG
Eine solche Matrix heißt ein \ul{Jordan-Kästchen}.\\
In der Basis \(B'=(\phi^{s-1}(v),\phi^{s-2}(v),\dots,v)\) gilt \(M_{B'}^{B'}(\phi)\)=TODO MATRIX (Dies ist das üblichere Jordan-Kästchen).\\
\proof
"(a)" Nach Teil (b) des Satzes folgt \(U_1=\dots=U_{s-1}=\{0\}\). Damit vereinfacht sich die Jordan-Zerlegung wie angegeben.
"(b)" folgt aus Teil (b) des Satzes und (a).
\qed
\subsubsection{Bemerkung}
Sei \(\delta_s\geq 1\) und \(\{v_1,\dots,v_\delta\}\) eine Basis von \(U_s\). Dann ist \(B=\{v_1,\phi(v_1),\dotsm\phi^{s-1}(v_1)\}\cup\dots\cup\{v_\delta,\phi(v_\delta),\dotsm\phi^{s-1}(v_\delta)\}\) eine Basis von \(W=U_s\oplus\phi(U_s)\oplus\dots\oplus\phi^{s-1}(U_s)\). Die Matrix von \(\phi|W\) bzgl. \(B\) ist blockdiagonal mit \(\delta_s\) Jordan-Kästchen auf der Hauptdiagonalen \(M_B^B(\phi|W)\)=TODO MATRIX  TODO CAPREV u.\\
\subsubsection{Satz (Die Jordansche Normalform)}
Sei \(\phi\in End_K(V)\) mit linearen Eigenfaktoren, also mit \(\chi_\phi(x)=(x-\lambda_1)^{a_1}\cdot\dots\cdot(x-\lambda_s)^{a_s}\). Dann gibt es eine Basis \(B\) in \(V\), so dass gilt: \(M_B^B(\phi)=\begin{pmatrix}
\lambda_1 I_{a_1}N_1 & & 0 \\
 & \ddots & \\
0 & & \lambda_s I_{a_s}N_s
\end{pmatrix}\) mit \(\lambda_i\cdot I_{a_i}+N_i=\begin{pmatrix}
\mbox{\fbox{\(\begin{matrix}
		\lambda_i & 1 & & 0\\
		& \ddots & \ddots & \\
		& & \ddots & 1 \\
		0& & & \lambda_i
		\end{matrix}\)}} & & \mbox{\Huge0} \\
	 & \ddots & \\
	 \mbox{\Huge0} & & \mbox{\fbox{\(\begin{matrix}
	 		\lambda_i & 1 & & 0\\
	 		& \ddots & \ddots & \\
	 		& & \ddots & 1 \\
	 		0& & & \lambda_i
	 		\end{matrix}\)}}
\end{pmatrix}\), wobei die Größe der \ul{Jordan-Blöcke} \fbox{\(\begin{matrix}
\lambda_i & 1 & & 0\\
& \ddots & \ddots & \\
& & \ddots & 1 \\
0& & & \lambda_i
\end{matrix}\)} durch die Jordan-Zerlegung von \((\phi-\lambda_iid_V)\Big|_{G_i}\) mit \(G_i=Gen(\phi,\lambda_i)\) gegeben ist.
\subsection{Diagonalisierbarkeit und Triagonalisierbarkeit (Buch: §4.3-4.4)}
Sei \(K\) ein Körper, \(V\) ein endlich-dimensionaler \(K\)-Vektorraum, \(\phi\in End_K(V)\).\\
\(\phi\) heißt diagonalisierbar \(\Leftrightarrow\) es gibt eine Basis von V bestehend aus Eigenvektoren von \(\phi \Leftrightarrow\) es gibt eine Basis \(B\) von \(V\), so dass \(M_B^B(\phi)\) eine Diagonalmatrix ist.
\subsubsection{Satz (Zweite Charakterisierung diagonalisierbarer Endomorphismen)}
Die folgenden Bedingungen sind äquivalent:
(a) \(\phi\) ist diagonalisierbar
(b) Alle Eigenfaktoren von \(\phi\) sind linear und \(Eig(\phi,\lambda)=Gen(\phi,\lambda)\) für alle Eigenwerte \(\lambda\) von \(\phi\)
(c) \(\mu_\phi(x)\) zerfällt in ein Produkt paarweise verschiedener Faktoren \(x-\lambda_i\).\\
\proof
"(a)\(\Rightarrow\)(b)" Sei \(B\) eine Basis von \(V\), so dass \(M_B^B(\phi)\) eine Diagonalmatrix ist. Dann ist \(\chi_\phi(x)=(x-\lambda_1)^{a_1}\cdot\dots\cdot(x-\lambda_s)^{a_s}\) mit \(\lambda_i\neq\lambda_j\) und \(a_i\geq 1\) und \(M_B^B(\phi)\)=TODO MATRIX und \(B=B_1\cupdot\dots\cupdot B_s\) und \(Eig(\phi,\lambda_i)=<B_i>_K=Gen(\phi,\lambda_i)\)
"(b)\(\Rightarrow\)(c)" \ul{z.z.} \(m_i=1\) für \(1,\dots,s\) (vgl. Übungsblatt 3, Aufgabe 7).
"(c)\(\Rightarrow\)(a)" Schreibe \(\mu_\phi(x)=(x-\lambda_1)\cdot\dots\cdot(x-\lambda_s)\) mit \(\lambda_i\neq\lambda_j\) für \(i\neq j\).\\
Nach Übungsblatt 3, Aufgabe 7 gilt \(Eig(\phi,\lambda_i)=Gen(\phi,\lambda_i)\) für \(i=1,\dots,s\).\\
Dann folgt \(V=Gen(\phi,\lambda_1)\oplus\dots\oplus Gen(\phi,\lambda_s)=Eig(\phi,\lambda_1)\oplus\dots\oplus Eig(\phi,\lambda_s)\). Somit hat \(V\) eine Basis bestehend aus Eigenvektoren von \(\phi\).
\qed
\subsubsection{Definition}
Der Endomorphismus \(\phi\) heißt \ul{triagonalisierbar}, wenn es eine Basis von \(V\) gibt, so dass \(M_B^B(\phi)\) eine obere Dreiecksmatrix ist.
\subsubsection{Satz (Charakterisierung triagonalisierbarer Endomorphismen)}
Genau dann ist \(\phi\) triagonalisierbar, wenn alle Eigenfaktoren von \(\phi\) linear sind.\\
\proof
"\(\Rightarrow\)" Sei \(B\) eine Basis von \(V\) bzgl. der \(M_B^B(\phi)\) eine obere Dreiecksmatrix ist. Schreibe \(M_B^B(\phi)=\)TODO MATRIX und erhalte \(\chi_\phi(x)=det(M_B^B(\phi))-x\cdot I_n)=(-1)^n(x-\lambda_1)\cdot\dots\cdot (x-\lambda_n)\).
"\(\Leftarrow\)" Dies folgt aus der Jordan-Zerlegung.
\qed
\section{Bilineare Abbildungen}
\setcounter{subsection}{24}
\subsection{Grundlegendes zu Bilinearformen (Buch §5.1,5.4)}
Sei \(K\) ein Körper und \(V\) ein endlich-dimensionaler \(K\)-Vektorraum mit \(n=dim_K(V)\).
\subsubsection{Definition}
(a) Sei \(W\) ein weiterer K-Vektorraum. Eine Abbildung \(\Phi:V\times V\rightarrow W\) heißt \ul{\(K\)-bilinear}, wenn für alle \(v,v_1,v_2,\tilde{v}, \tilde{v_1}, \tilde{v_2} \in V\) und \(a_1, a_2\in K\) gilt:
(a) \(\Phi(a_1v_1+a_2v_2,\tilde{v})=a_1\cdot\Phi(v_1,\tilde{v})+a_2\cdot\Phi(v_2,\tilde{v})\)
(b) \(\Phi(v,a_1\tilde{v_1}+a_2\tilde{v_2})=a_1\cdot\Phi(v,\tilde{v_1})+a_2\cdot\Phi(v,\tilde{v_2})\).
(b) Eine bilineare Abbildung \(\Phi:V\times V\rightarrow K\) heißt auch eine \ul{Bilinearform} auf V.
\subsubsection{Bemerkung (Längenmessung in \(\mathbb{R}^2\))}
Sei \(v=(a_1,a_2)\in\mathbb{R}^2\) mit \(a_1,a_2\in\mathbb{R}\). Dann heißt \(l(v)=||v|| = \sqrt{a_1^2+a_2^2}\) die \ul{Länge} (oder die \ul{Norm}) von \(v\).
\subsubsection{Definition}
Sei \(v=(a_1,a_2)\in\mathbb{R}^2\backslash\{0\}\). Dann heißt \(e_v=\frac{v}{||v||}=(\frac{a_1}{\sqrt{a_1^2+a_2^2}},\frac{a_2}{\sqrt{a_1^2+a_2^2}})\) der \ul{Einheitsvektor} in \ul{Richtung} \(v\).
\subsubsection{Bemerkung (Winkelmessung in \(\mathbb{R}^2\))}
Seien \(v=(a_1,a_2)\) und \(w=(b_1,b_2)\) zwei Vektoren in \(\mathbb{R}^2\backslash\{0\}\).\\
Gesucht ist eine Funktion \(\Phi:\mathbb{R}^2\times\mathbb{R}^2\rightarrow\mathbb{R}\), mit deren Hilfe wir den Winkel \(\alpha=\sphericalangle(v,w)\) zwischen \(v\) und \(w\) definieren können.
(a) Der Winkel \(\alpha\) sollte nicht von \(||v||\) und \(||w||\) abhängen. Es gilt: \(cos(\alpha)=\frac{||u||}{||w||}\), wobei \(u\) der Fußpunkt des Lots von \(w\) auf \(v\) ist.
(b) Gilt \(w\bot v\) (d.h. steht \(w\) senkrecht auf \(v\)), so gilt \(cos(\alpha)=0\) und \(u=0\).
(c) Genauer gilt: \(u=||u||\cdot e_v =||w||\cdot cos(\alpha)\cdot e_v\)
(d) Nach der Vektoraddition gilt \(w=u+\tilde{u}\)\\
Ferner gilt \(\tilde{u}\bot v\).
(1) Gilt \(\Phi(v,u+\tilde{u})=\Phi(v,u)+\underbrace{\Phi(v,\tilde{u})}_{=0 \text{ da } \tilde{u}\bot v}\), so folgt \(\Phi(v,w)=\Phi(v,u)\) und somit \(\Phi(v,w)=\Phi(\norm{v}\cdot e_v, ||w||\cdot cos(\alpha)\cdot e_v)\)
(e) Gilt (2) \(\Phi(\lambda \tilde{v}, \tilde{w})=\Phi(\tilde{v},\lambda \tilde{w})=\lambda\Phi(\tilde{v},\tilde{w})\), so folgt \(\Phi(v,w)=||v||\cdot||w||\cdot cos(\alpha)\cdot\underbrace{\Phi(e_v,e_v)}_{\text{Winkel }0\degree,\text{ }cos(\alpha)=1}\).
(f) Gilt (3) \(\phi(v,v)=||v||^2\), so folgt \(\Phi(e_v,e_v)=\Phi(\frac{v}{||v||},\frac{v}{||v||})\frac{1}{||v|||^2}\cdot||v||^2=1\) und damit \(\Phi(v,w)=||v||\cdot ||w||\cdot cos(\alpha)\), also \(cos(\alpha)= \frac{\Phi(v,w)}{||v||\cdot||w||}\).
Insgesamt sollte \(\Phi\) folgende Eigenschaften erfüllen:
(1) \(\Phi(v,w_1+w_2)=\Phi(v,w_1)+\Phi(v,w_2)\) für \(v,w_1,w_2\in\mathbb{R}^2
\Phi(v_1,+v_2w)=\Phi(v_1,w)+\Phi(v_2,w)\) für \(v_1,v_2,w\in\mathbb{R}^2\)
(2) \(\Phi(\lambda v,w)=\Phi(v,\lambda w)=\lambda\Phi(v,w)\) für \(\lambda\in\mathbb{R}\) und \(v,w\in\mathbb{R}^2\)
(3) \(\Phi(v,w)=0\) falls \(v \bot w\)
(4) \(\Phi(v,v)=||v||^2\) für \(v\in\mathbb{R}^2\)
Kann man hieraus \(\Phi\) bestimmen? Für \(v=(a_1,a_2)\) und \(w=(b_1,b_2)\) folgt:
\(\Phi(v+w,v+w)=||v+w||^2=||(a_1+b_1,a_2+b_2)||^2=(a_1+b_1)^2+(a_2+b_2)^2=a_1^2+a_2^2+b_1^2+b_2^2+2(a_1b_1+a_2b_2)\)\\
\(\Phi(v+w,v+w)=\Phi(v,v)+\Phi(v,w)+\underbrace{\Phi(w,v)}_{=\Phi(v,w)}+\Phi(w,w)=||v||^2+||w||^2+2\Phi(v,w)=a_1^2+a_2^2+b_1^2+b_2^2+2\Phi(v,w) \Rightarrow \Phi(v,w)=a_1b_1+a_2b_2\).
Wie man leicht nachprüft, erfüllt dieses \(\Phi\) die Eigenschaften (1)-(4).
Der Winkel \(\alpha=\sphericalangle(v,w)\) ist dann durch \(cos(\alpha)=\frac{\Phi(v,w)}{||v||\cdot||w||}\) definiert.
\subsubsection{Definition}
Sei \(B=(v_1,v_2,\dots,v_n)\) ein Basistupel von \(V\).
Dann ist die Abbildung \(\Phi :\underset{(a_1v_1+\dots+a_nv_n,b_1v_1+\dots+b_nv_n)\mapsto a_1b_1+a_2b_2+\dots+a_nb_n}{V\times V\rightarrow K}\) eine Bilinearform auf \(V\). Sie heißt die \ul{Standardbilinearform} auf \(V\) bzgl. der Basis \(B\).
\subsubsection{Definition}
Sei \(B=(v_1,\dots,v_n)\) ein Basistupel von \(V\).
(a) Ist \(\Phi:V\times V\rightarrow K\) eine Bilinearform auf \(V\), so heißt die Matrix \(G_B^B(\Phi)=(\phi(v_i,v_j))_{i,j}\in Mat_n(K)\) die \ul{Gramsche Matrix} (oder die \ul{Strukturmatrix}) von \(\Phi\).
(b) Die Menge aller Bilinearformen auf \(V\) wird mit \(Bil_K(V)\) bezeichnet.
\subsubsection{Satz}
Sei \(B=(v_1,\dots,v_n)\) ein Basistupel von \(V\).
(a) Zu jeder Matrix \(A=(a_{ij})\in Mat_n(K)\) definiere \(\Phi_A:V\times V\rightarrow K\) durch \(\Phi_A(b_1v_1+\dots+b_nv_n,c_1v_1+\dots+c_nv_n)=\sum_{i=1}^{n}\sum_{j=1}^{n}b_ic_ja_{ij}\). Dann ist \(\Phi_A\) eine Bilinearform auf \(V\) und \(G_B(\Phi_A)=A\).
(b) Die beiden Zuordnungen \(\Phi_A\rightarrow G_B(\Phi)\) und \(A\rightarrow \Phi_A\) sind invers zueinander.
(c) Die Menge \(Bil_K(V)\) ist bzgl. \((\Phi+\Psi)(v,w)=\Phi(v,w)+\Psi(v,w)\) und \((\lambda\Phi)(v,w)=\lambda\cdot\Phi(v,w)\) für \(v,w\in V\) und \(\lambda\in K\) ein \(K\)-Vektorraum.
(d) Die Abbildung \(\phi:\underset{\Phi\mapsto G_B(\Phi)}{Bil_K(V)\rightarrow Mat_n(K)}\) ist ein Isomorphismus von \(K\)-Vektorräumen.
(e) Die Standardbilinearform \(\Phi\) erfüllt \(G_B(\phi)=I_n\).\\
\proof
"(a)" \ul{z.z.} \(\Phi_A\) ist \(K\)-bilinear. Beachte: \(\Phi(b_1v_1+\dots+b_nv_n, c_1v_1+\dots+c_nv_n)=(b_1,\dots,b_n)\cdot A\cdot \begin{pmatrix}
c_1\\
\vdots\\
c_n
\end{pmatrix}\).
Gilt \(v=v'+v''\) mit \(v'=b_1'v_1+\dots+b_n'v_n\) und \(v''=b_1''v_1+\dots+b_n''v_n\) und \(w=c_1v_1+\dots+c_nv_n\) so folgt:
\(\Phi(v'+v'',w)=(b_1'+b_1'',\dots,b_n'+b_n'')\cdot A \cdot \begin{pmatrix}
c_1\\
\vdots\\
c_n
\end{pmatrix}=(b_1',\dots,b_n')\cdot A \cdot \begin{pmatrix}
c_1\\
\vdots\\
c_n
\end{pmatrix}+(b_1'',\dots,+b_n'')\cdot A \cdot \begin{pmatrix}
c_1\\
\vdots\\
c_n
\end{pmatrix}=\Phi(v',w)+\Phi(v'',w)\).\\
Analog rechnet man die anderen Bedingungen nach.\\
Die Gramsche Matrix \(G_B(\Phi_A)\) hat als \((a_{ij})\)-Eintrag gerade \(\Phi(v_i,v_j)=e_i^{tr}\cdot A \cdot e_j=e_i^{tr}\cdot \underbrace{A\cdot e_j}_{j\text{-te Spalte von} A}=a_{ij}\) also der \((i,j)\)-Eintrag von \(A\).\\
Es folgt \(G_B(\Phi_A)=A\).
"(b)" Soeben gezeigt ist \(G_B(\Phi_A)=0\). Die Behauptung \(\Phi_{G_B(\Phi)}=\Phi\) folgt, wenn wir zeigen, dass für alle \(i,j=1,\dots,n\) gilt:\(\Phi_{G_B(\Phi)}(v_i,v_j)=\Phi(v_i,v_j)\). Wie gesehen ist \(\phi_{G_B(\Phi)}(v_i,v_j)\) gerade der \((i,j)\)-Eintrag der Matrix \(G_B(\Phi)\). Dieser ist nach Definition von \(G_B(\Phi)\) gleich \(\Phi(v_i,v_j)\).
"(c)" \ul{z.z.} \(\Phi+\Psi,\lambda\cdot\Phi\) sind Bilinearformen: Nachrechnen!\\
\ul{z.z.} \(Bil_K(V)\) erfüllt bzgl. \(+,\cdot\) die Vektorraumaxiome: Nachrechnen!
"(d)" Die Bijektivität von \(\phi\) folgt aus (b).\\
\ul{z.z.} \(\phi\) ist \(K\)-linear: Seien \(\Phi,\Psi\in Bil_K(V)\) und \(\lambda,\mu\in K\). Dann gilt \(\phi(\lambda\Phi+\mu\Psi)=((\lambda\Phi+\mu\Psi)(v_i,v_j))_{i,j}=(\lambda\Phi(v_i,v_j)+\mu\Psi(v_i,v_j))_{i,j}=\lambda\cdot(\Phi(v_i,v_j))_{i,j}+\mu\cdot(\Psi(v_i,v_j))_{i,j}=\lambda\phi(\Phi)+\mu\phi(\Psi)\).
"(e)" Für \(\Phi(a_1v_1+\dots+a_nv_n,b_1v_1+\dots+b_nv_n)=a_1b_1+\dots+a_nb_n\) gilt \(\Phi(v_i,v_j)=e_i^{tr}e_j=\rho_{ij}=\begin{cases}
1,& \text{falls }i=j\\
0,& \text{sonst}
\end{cases}\), also \(G_B(\Phi)=(\rho_{ij})=I_n\).
\qed
\ul{Frage:} Was passiert bei einem Basiswechsel mit der Gramschen Matrix?
\subsubsection{Satz}
Seien \(B=(v_1 \dots v_n)\) und \(V=(w_1 \dots w_m)\) zwei Basen von \(V\) und sei \(T_C^B=(t_{ij})\in Mat_n(K)\) die Transformationsmatrix (d.h. die j-te Spalte von \(T_C^B\) enthält die Koordinaten von \(v_j\) in der Basis \(C\)). Ferner sei \(\Phi:V\times V\rightarrow K\) eine Bilinearform.\\
Dann gilt: \(G_B(\Phi)=(T_C^B)^{tr}\cdot G_C(\Phi)\cdot T_C^B\).\\
\proof
Für \(v,w\in V\) schreibe \(v=a_1v_1+\dots+a_nv_n=a_1'w_1+\dots+a_n'w_n und w=b_1v_1+\dots+b_nv_n=b_1'w_1+\dots+b_n'w_n \) mit \(a_i,a_j',b_k,b_l'\in K\). Dann
\(TODO MATRIX
=T_C^B\cdot TODO MATRIX\) und \(TODO MATRIX = T_C^B\cdot TODO MATRIX\). Dann gilt: \(\Phi(v,w)=(a_1,\dots,a_n)G_B(\Phi) TODO MATRIX = (a_1',\dots,a_n')\cdot G_C(\Phi) TODO MATRIX=(a_1',\dots,a_n')=(a_1,\dots,a_n)\cdot (T_C^B)^{tr} =(a_1,\dots,a_n)(T_C^B)^{tr}\cdot G_C(\Phi)\cdot T_C^B TODO MATRIX\). Setzt man \((a_1,\dots,a_n)=e_i\) und \((b_1,\dots,b_n)=e_j\), so ist \(e_i^{tr}\underbrace{Me_j}_{j\text{-te Spalte von } M}=(i,j)-\)Eintrag von \(M\). Für alle \(i,j=1,\dots,n\) ist also der \((i,j)\)-Eintrag von \(G_B(\Phi)\) gleich dem \((i,j)\)-Eintrag von \((T_C^B)^{tr}\cdot G_C(\Phi)T_C^B\)
\qed
\subsubsection{Bemerkung}
Ist \(\phi\in End_K(V) \), so gilt \(M_B(\phi)=(T_C^B)^{-1}\cdot M_C(\phi)\cdot T_C^B\) und \(G_B(\phi)=(T_C^B)^{tr}\cdot G_C(\phi)\cdot T_C^B\).
\subsubsection{Definition}
(a) Eine Bilinearform \(\Phi:V\times V\rightarrow K\) heißt \ul{symmetrisch}, wenn für alle \(v,w\in V\) gilt: \(\Phi(v,w)=\Phi(w,v)\)
(b) Eine Bilinearform \(\Phi:V\times V\rightarrow K\) heißt \ul{antisymmetrisch}, wenn für alle \(v,w\in V\) gilt: \(\Phi(v,w)=-\Phi(w,v)\)
\subsubsection{Satz}
Eine Bilinearform \(\Phi:V\times V\rightarrow K\) ist genau dann symmetrisch, wenn \(G_B(\Phi)\) eine symmetrische Matrix ist, d.h. wenn \(G_B(\Phi)=G_B(\Phi)^{tr}\) gilt.\\
\proof
"\(\Rightarrow\)" Für \(i,j=1\dots n\) gilt \(\underbrace{\Phi(v_i,v_j)}_{(i,j)\text{-Eintrag von } G_B(\Phi)}=\underbrace{\Phi(v_j,v_i)}_{(i,j)\text{-Eintrag von } G_B(\Phi)^{tr}}\).
"\(\Leftarrow\)" Sei \(v=a_1v_1+\dots+a_nv_n\) und \(w=b_1v_1+\dots+b_nv_n\) mit \(a_i,b_j\in K\). Dann gilt \(\Phi(v,w)=\sum_{i=1}^{n}\sum_{j=i}^{n}a_ib_j\cdot\Phi(v_i,v_j)=\sum_{i=1}^{n}\sum_{j=i}^{n}a_ib_j\Phi(v_j,vi)=\Phi(w,v)\)
\qed
\ul{Beispiel:} Die Standardbilinearform \(\Phi\) ist symmetrisch, denn \(G_\chi(\Phi)=I_n\) ist symmetrisch.
\subsubsection{Definition}
Sei \(\Phi:V\times V\rightarrow K\) eine Bilinearform.
(a) Zwei Vektoren \(v,w\in V\) heißen \ul{orthogonal} bzgl. \(\Phi\), wenn gilt: \(\Phi(v,w)=0\). \ul{Schreibweise:} \(v\bot_\Phi w\)
(b) Ist \(v\in V\) und \(U\subseteq V\), so heißt \(v\) \ul{orthogonal} zu \(U\), wenn für alle \(u\in U\) gilt \(v\bot_\Phi u\).
(c) Für \(U,W\subseteq V\) sagen wir, dass \(U\) bzgl.\(\Phi\) \ul{orthogonal} ist zu \(W\), falls für alle \(u\in U, w\in W\) gilt: \(u\bot_\Phi w\). \ul{Schreibweise:} \(U\bot_\Phi W\).
\subsubsection{Beispiel}
Ein Vektor \(v\neq 0\) kann zu sich selbst orthogonal sein und zwar sogar bzgl. der Standardbilinearform.\\
Betrachte z.B. \(K=\mathbb{F}_p\) mit \(p\) prim und \(\Phi:\mathbb{F}_p^p\times \mathbb{F}_p^p\rightarrow \mathbb{F}_p\) Standardbilinearform. Dann gilt \(\Phi(\underbrace{(1,1,\dots,1)}_{p},\underbrace{(1,1,\dots,1)}_{p})=1\cdot 1+\dots+1\cdot 1=0\).
\subsubsection{Beispiel}
Ein Vektor \(v\neq 0\) kann zu ganz \(V\) orthogonal sein!\\
Betrachte z.B. \(\Phi:K^2\times K^2\rightarrow K\) mit \(G_\chi(\Phi)=\begin{pmatrix}
1 & -1\\
-1 & 1
\end{pmatrix}\).
Für \(v=(1,1)\) und \(w=(1_1,a_2)\) gilt dann \(\Phi(v,w)=(1,1)\cdot\begin{pmatrix}
1 & -1\\
-1 & 1
\end{pmatrix}\begin{pmatrix}
a_1\\
a_2
\end{pmatrix}=(0,0)\cdot\begin{pmatrix}
a_1\\
a_2
\end{pmatrix}=0\). Also folgt \(v\bot_\Phi K^2\).
\subsubsection{Beispiel}
Sei \(\Phi:\mathbb{R}^2\times\mathbb{R}^2\rightarrow\mathbb{R}\) die Standardbilinearform. Für \(v=(a_1,a_2)\) und \(w=(b_1,b_2)\) gilt \(v\bot_\Phi w\) genau dann, wenn \(a_1b_1+a_2b_2=0\) erfüllt ist. TODO SKIZZE
O.E. sei \(a_2\neq 0\). Dann folgt \(b_2=-\frac{a_1}{a_2}b_1\). Dies liefert \((b_1,b_2)=(b_1,-\frac{a_1}{a_2}b_1)=\underbrace{a_2^{-1}b_1}_{\neq 0}\cdot(a_2,-a_1)\).
\subsubsection{Definition}
Eine Bilinearform \(\Phi:V\times V\rightarrow K\) heißt \ul{nicht ausgeartet}, wenn für \(v\in V\) aus \(v\bot_\Phi V\) folgt, dass \(v=0\) gilt.
\subsubsection{Satz}
Sei \(B=(v_1\dots v_n)\) ein Basistupel von \(V\). genau dann ist \(\Phi\) nicht ausgeartet, wenn \(det(G_B(\Phi))\neq 0\) gilt.\\
\proof
Sei \(v\in V\). Schreibe \(v=a_1v_1+\dots+a_nv_n\) mit \(a_i\in K\). Dann gilt: \(v\bot_\Phi V\Leftrightarrow \Phi(v,v_1)=\Phi(v,v_2)=\dots=\Phi(v,v_n)=0\Leftrightarrow \begin{cases}
a_1\Phi(v_1,v_1)+\dots+a_n\Phi(v_n,v_1)=0\\
\vdots\\
a_1\Phi(v_1,v_n)+\dots+a_n\Phi(v_n,v_n)=0
\end{cases}\)\\
Also folgt: \(\Phi\) nicht ausgeartet \(\Leftrightarrow\) aus \(v\bot_\Phi V\) folgt \(v=0\) \(\Leftrightarrow\) das einzige Tupel \((a_1,\dots,a_n)\), das \((*)\) erfüllt ist \((a_1,\dots,a_n)=(0,\dots,0)\)\\
\(\Leftrightarrow\) \(det(G_B(\Phi))^{tr}\neq 0\) \(\Leftrightarrow\) \(det(G_B(\Phi))\neq 0\).
\qed
\subsubsection{Beispiel}
(a) Die Standardbilinearform \(\Phi:K^n\times K^n\rightarrow K\) ist nicht ausgeartet, denn \(det(G_\chi(\Phi))=det(I_n)=1\neq 0\).
(b) Die Matrix \(\begin{pmatrix}
	1 & -1\\
	-1 & 1
\end{pmatrix}\) definiert eine ausgeartete Bilinearform auf \(K^2\), da \(det\begin{pmatrix}
1 & -1\\
-1 & 1
\end{pmatrix}\neq 0\).
\subsubsection{Definition}
Sei \(\Phi:V\times V\rightarrow K\) eine Bilinearform und \(U\subseteq V\) ein \(K\)-Untervektorraum.
(a) Ein Untervektorraum \(W\subseteq V\) mit \(U\bot_\Phi W\) und \(U\oplus W=V\) heißt ein \ul{orthogonales Komplement} von \(U\).
(b) Die Menge \(U^\bot=\{v\in V|v\bot_\Phi U\}\) heißt der zu \(U\) \ul{orthogonale Raum}.
\subsubsection{Satz (Eigenschaften des orthogonalen Untervektorraums)}
Sei \(\Phi:V\times V\rightarrow K\) eine Bilinearform und seien \(U,W\) \(K\)-Untervektorräume von \(V\).
(a) \(U^\bot\) ist ein \(K\)-Untervektorraum von \(V\).
(b) Ist \(U=<v>\) mit \(v\in V\backslash\{0\}\), so gilt \(U^\bot=<v>^\bot=<av>^\bot\) für alle \(a\in K\backslash\{0\}\),
(c) Gilt \(U\subseteq W\), so folgt \(W^\bot\subseteq U^\bot\).
(d) Ist \(S\subseteq V\) eine Teilmenge, so gilt \(S^\bot=\{v\in V|v\bot_\Phi S\}=<S>_K^\bot\).
(e) Ist \(\Phi\) symmetrisch, so gilt \((U^\bot)^\bot\supseteq U\) und \(((U^\bot)^\bot)^\bot=U^\bot\).\\
\proof
"(a)" Seien \(v_1,v_2\in U^\bot\) und \(a_1,a_2\in K\). Für alle \(u\in U\) gilt dann \(\Phi(a_1v_1+a_2v_2,u)=a_1\Phi(v_1,u)+a_2\Phi(v_2,u)=0\) und somit \(a_1v_1+a_2v_2\in U^\bot\).
"(b)" "\(\subseteq\)" Ist \(w\in<v>^\bot\), also \(\Phi(w,cv)=0\) für alle \(c\in K\), so folgt \(\Phi(w,b\cdot (av))=0\) für alle \(b\in K\), aso \(w\in<av>^\bot\).
"\(\supseteq\)" ist \(w\in<av>^\bot\), also \(\Phi(w,cav)=0\) für alle \(c\in K\), so folgt (mit \(c=a^{-1}\)), dass \(\Phi(w,v)=0\) gilt. Dies liefert \(\Phi(w,bv)=0\) für alle \(b\in K\backslash\{0\}\), also \(w\in<v>^\bot\).
"(c)" Für \(v\in W^\bot\) und \(u\subseteq W\) gilt \(\Phi(v,u)=0\) und somit \(v\in U^\bot\).
"(d)" "\(\supseteq\)" klar "\(\subseteq\)" Sei \(v\in S^\bot\) und \(w\in<S>\). Schreibe \(w=a_1s_1+\dots+a_rs_r\) mit \(a_i\in K\) und \(s_i\in S\) und erhalte \(\Phi(v,w)=a_1\underbrace{\Phi(v,s_1)}_{=0}+\dots+a_r\underbrace{\Phi(v,s_r)}_{=0}=0\). Dies liefert \(v\in<S>^\bot\).
"(e)" Für \(u\in U\) gilt: \(\Phi(v,u)=0\) für alle \(v\in U^\bot\), also \(\Phi(u,v)=0\) für alle \(v\in U^\bot\). Dies zeigt \(u\in(U^\bot)^\bot\), also \(U\subseteq(U^\bot)^\bot\). Nach (e) gilt \(U^\bot\subseteq ((U^\bot)^\bot)^\bot\). Mit (c) folgt aus \(U\subseteq ((U^\bot)^\bot)^\bot\subseteq U^\bot\) gilt.
\qed
\subsubsection{Satz (Dimensionsformel für orthogonale Untervektorräume)}
Sei \(\Phi:V\times V\rightarrow V\) eine nicht ausgeartete Bilinearform und \(U\subseteq V\) ein \(K\)-Untervektorraum.\\
Dann gilt: \(dim_K(U)+dim_K(U^\bot)=dim_K(V)\).\\
(\(\lightning\) Im Allgemeinen gilt \ul{nicht} \(U\cap U^\bot=\{0\}\))\\
\proof
Sei \((u_1,\dots, u_m)\) ein Basistupel für \(U\) und \(C=(w_1,\dots,w_k)\) ein Tupel von Vektoren, so dass \(B\cup C\) ein Basistupel von \(V\) ist. Sei \(v_1,\dots,v_n)\) dieses Basistupel, d.h \(v_i=u_i\) für \(i=1,\dots,m\) und \(v_{m+j}=w_j\) fpr \(j=1,\dots,k\) (mit \(m+k=n\)).\\
\ul{z.z.:} \(dim_K(U^\bot)=k\).\\
Fpr \(v=a_1v_1+\dots+a_nv_n=(a_1v_1+\dots+a_mv_m)+(a_{m+1}v_{m+1}+\dots+a_{m+k}v_{m+k})\) gilt: \(v\in U^\bot\) \(\Leftrightarrow\) \(\begin{cases}
	\Phi(v,v_1)=0\\
	\vdots\\
	\Phi(v,v_m)=0
	\end{cases}\) \(\Leftrightarrow\) \(\begin{cases}
	a_1\Phi(v_1,v_1)+\dots+a_n\Phi(v_n,v_1)=0\\
	\vdots\\
	a_1\Phi(v_1,v_m)+\dots+a_n\Phi(v_n,v_m)=0
	\end{cases}\)
	Dies ist ein lineares Gleichungssystem in den Unbestimmten \(a_1,\dots,a_n\) bestehend aus \(m\) Gleichungen.\\
	\ul{z.z.:} Der Rang der Koeffizientenmatrix ist der maximale, also \(m\).\\
	DOe Koeffizientenmatrix besteht aus den ersten \(m\) Zeilen von \(G_B(\Phi)^{tr}\). Wegen \(det(G_B(\Phi)^{tr})=det(G_B(\Phi))\neq 0\) sind diese \(m\) Zeilen linear unabhängig.
	\qed
\subsubsection{Korollar}
Ist \(\Phi:V\times V\rightarrow K\) eine nicht ausgeartete, symmetrische Bilinearform, so gilt: \((U^\bot)^\bot=U\) für jeden \(K\)-Untervektorraum \(U\) von \(V\).\\
\proof
Da \(\Phi\) symmetrisch ist, gilt \(U^\bot)^\bot\underset{TODO STERN KREIS}{\supset} U\). Da \(\Phi\) nicht ausgeartet ist, gilt \(dim_K(U^\bot)^\bot=dim_K(V)-dim_K(U^\bot)=dim_K(V)-(dim_K(V)-dim_K(U))=dim_K(U)\). Also ist TODO STERN KREIS eine Gleichheit.
\qed
\subsubsection{Beispiel}
(a) Sei \(K=\mathbb{R}\) und \(\Phi:\underset{((a_1,a_2,a_3),(b_1,b_2,b_3))\mapsto a_1b_2+a_2b_2+a_3+b_3}{\mathbb{R}^2\times \mathbb{R}^2\rightarrow \mathbb{R}}\) die Standardbilinearform. Für eine Gerade \(G=\mathbb{R}\cdot (a,b)\) durch 0 mit \((a,b)\neq (0,0)\) gilt \(G^\bot=\mathbb{R}\cdot (-b,a)\) und \(dim_\mathbb{R}(G)=dim_\mathbb{R}(G^\bot)=1\).
(b) Sei \(K=\mathbb{R}\) und \(V=\mathbb{R}^3\) und \(\Phi:\mathbb{R}^3\times\mathbb{R}^3\rightarrow \mathbb{R}\) die Standardbilinearform.\\
Ist \(G=\mathbb{R}\cdot (a_1,a_2,a_3)\) mit \((a_1,a_2,a_3)\neq (0,0,0)\) eine Gerade durch \(0\), so gilt: \(G^\bot=\{(b_1,b_2,b_3)\in\mathbb{R}^3|a_1b_1+a_2b_2+a_3b_3=0\}=<(-a_2,a_1,0),(0,-a_3,a_2),(-a_3,0,a_1)>\).\\
Falls \(a_1\neq0\), so gilt \(G^\bot=<(-\frac{a_2}{a_1},1,0),(-\frac{a_3}{a_1},0,1)>\)\\
\(dim_\mathbb{R}(G^\bot)=2=dim\mathbb{R}(\mathbb{R}^3)-dim_\mathbb{R}(G)\)\\
\ul{Probleme:}
(1) Es kann ein \(v\in V\backslash\{0\}\) geben mit \(\Phi(v,v)=0\).
(2) Es kann \(U\cap U^\bot\neq \{0\}\).
\subsection{Skalarprodukte (vgl. Buch S. 288-299)}
Im Folgenden sei \(K=\mathbb{R}\). Sei \(V\) ein (endlich-dimensionaler) \ul{reeller Vektorraum}, d.h. ein \(\mathbb{R}\)-Vektorraum ud sei \(n=dim_K(V)\).
\subsubsection{Definition}
(a) Eine Bilinearform \(\Phi:V\times V\rightarrow \mathbb{R}\) heißt \ul{positiv definit}, wenn gilt: \(\begin{cases}
\Phi(v,v)\geq 0\text{ für alle }v\in V\\
\Phi(v,v)=0 \Rightarrow v=0
\end{cases}\).
(b) Ein \ul{Skalarprodukt} auf \(V\) ist eine symmetrische, positiv definite Bilinearform \(\Phi:V\times V\rightarrow \mathbb{R}\).
(c) Ein \(\mathbb{R}\)-Vektorraum zusammen mit einem Skalarprodukt heißt ein \ul{euklidischer Vektorraum}.
\subsubsection{Bemerkung}
Ein Skalarprodukt ist nicht ausgeartet: Ist \(v\in V\) mit \(v\bot_\Phi V\) so folgt \(\Phi(v,v)=0\) und somit \(v=0\). Ein euklidischer Vektorraum enthält keine \ul{isotropen} Vektoren \(\neq 0\) (also \(\Phi(v,v)=0\Rightarrow v=0\))
\subsubsection{Beispiel}
Die Standardbilinearform \(\underset{((a_1\dots a_n),(b_1\dots b_n))\mapsto a_1b_1+\dots a_nb_n}{\Phi:\mathbb{R}^n\times \mathbb{R}^n\rightarrow \mathbb{R}}\) ist ein Skalarprodukt:\\
\(\Phi((a_1\dots a_n),(a_1\dots a_n))=a_1^2+\dots+a_n^2\geq 0\) und \(a_1^2+\dots+a_n^2=0\Rightarrow a_1=\dots=a_n=0\).\\
Es heißt auch das \ul{Standardskalarprodukt}. Wir schreiben auch \(<v,w>\) statt \(\Phi(v,w)\).\\
\ul{Fragen:} (1) Gibt es weitere Skalarprodukte auf \(\mathbb{R}^n\)?
(2) Wie kann man anhand von \(G_B(\Phi)\) erkennen, ob \(\Phi\) positiv definiert ist?
\subsubsection{Definition}
Sei \(V\) ein reeller Vektorraum.
(a) Eine \ul{Norm} auf \(V\) ist eine Abbildung \(\norm{\cdot}:V\rightarrow \mathbb{R}\) mit folgenden Eigenschaften:
(1) \(\norm{v}\geq 0\) und aus \(\norm{v}=0\) folgt \(v=0\) für alle \(v\in V\)
(2) Für \(a\in \mathbb{R}\) gilt \(\norm{a\cdot v}=\abs{a}\cdot \norm{v}\) für alle \(v\in V\)
(3) \(\norm{v+w}\leq \norm{v}+\norm{w}\) für alle \(v,w\in V\) (Dreiecksungleichung)
(b) Ein reeller Vektorraum mit einer Norm heißt ein \ul{normierter Vektorraum}.
\subsubsection{Definition}
Sei \((V,\Phi)\) ein euklidischer Vektorraum. Dann heißt \(\norm{\cdot}_\Phi:\underset{v\mapsto \sqrt{\Phi(V,V)}}{V\rightarrow \mathbb{R}}\) die zu \(\Phi\) \ul{assoziierte Norm}.
(b) Die zum Standardskalarprodukt auf \(\mathbb{R}^n\) assoziierte Norm heißt die \ul{euklidische Norm} auf \(\mathbb{R}^n\).
\subsubsection{Satz}
Sei \(V\) ein reeller Vektorraum und \(\Phi:V\times V\rightarrow \mathbb{R}\) ein Skalarprodukt auf \(V\).
(a) (Cauchy-Schwarzsche Ungleichung) \(\abs{\Phi(v,w)}\leq \norm{v}_\Phi\cdot \norm{w}_\Phi\) für alle \(v,w\in V\).
(b) Die zu \(\Phi\) assoziierte Norm \(\norm{\cdot}_\Phi\) ist eine Norm auf \(V\). Insbesondere gilt: \(\norm{v+w}\leq \norm{v}_\Phi+\norm{w}_\Phi\) für \(v,w\in V\).\\
\proof
"(a)" Für \(w=0\) sind beide Seiten gleich \(0\). Sei also \(w\neq 0\). Für \(a\in\mathbb{R}\) gilt \(\Phi(v-aw,v-aw)=\Phi(v,v)-a\cdot \Phi(v,w)-a\cdot \Phi(w,v)+a^2\cdot \Phi(w,w)=\Phi(v,v)-2a\cdot \Phi(v,w)+a^2\phi(w,w)\geq 0\). Wegen \(w\neq 0\) gilt \(\Phi(w,w)>0\) und wir können \(a=\frac{\Phi(v,w)}{\Phi(w,w)}\) wählen.\\
Dann folgt: \(\Phi(v,v)=\frac{2\Phi(v,w)^2}{\Phi(w,w)}+\frac{\Phi(v,w)^2}{\Phi(w,w)^2}\cdot \Phi(w,w)=\Phi(v,v)-\frac{\Phi(v,w)^2}{\Phi(w,w)}\geq 0\Rightarrow \Phi(v,v)\Phi(w,w)\geq \Phi(v,w)^2\) und da beide Seiten positiv sind, folgt \(\abs{\Phi(v,w)}\leq \sqrt{\Phi(v,v)}\sqrt{\Phi(w,w)}=\norm{v}_\Phi\cdot \norm{w}_\Phi\).
"(b)" Offenbar gilt \(\norm{v}_\Phi\geq 0\) und \(\norm{v}_\Phi=\sqrt{\Phi(v,v)}=0\Rightarrow\Phi(v,v)=0\Rightarrow v=0\).\\
Ferner gilt für \(a\in\mathbb{R}\) und \(v\in V\) die Gleichung \(\norm{av}_\Phi=\sqrt{\Phi(av,av)}=\sqrt{a^2\cdot \Phi(v,v)}=\abs{a}\cdot \sqrt{\Phi(v,v)}=\abs{a}\cdot \norm{v}_\Phi\).\\
Für \(v,w\in V\) gilt:\\
\(\norm{v+w}_\Phi^2=\Phi(v+w,v+w)=\Phi(v+v)+2\cdot \Phi(v,w)+\Phi(w,w)\leq \norm{v}_\Phi^2+\norm{w}_\Phi^2+2\abs{\Phi(v,w)}\underset{\text{c.s.}}{\leq}\norm{v}_\Phi^2+\norm{w}_\Phi^2+2\norm{v}_\Phi\cdot \norm{w}_\Phi=(\norm{v}_\Phi+\norm{w}_\Phi)^2\).\\
Da beide Seiten positiv sind, folgt \(\norm{v+w}_\Phi\leq \norm{v}_\Phi+\norm{w}_\Phi\)
\qed
\subsubsection{Definition}
Für alle \(v,w\in V\backslash\{0\}\) gilt: \(\frac{\abs{\Phi(v,w)}}{\norm{v}_\Phi\cdot \norm{w}_\Phi}\leq 1\), also \(-1\leq \frac{\Phi(v,w)}{\norm{v}_\Phi\cdot \norm{w}_\Phi}\leq 1\).\\
Damit ist der \ul{Winkel} \(\alpha\in[0,\pi[\) zwischen \(v\) und \(w\) eindeutig definiert durch \(cos(\alpha)=\frac{\Phi(v,w)}{\norm{v}_\Phi\cdot \norm{w}_\Phi}\)
\subsubsection{Bemerkung}
Mit Def. 26.7, angewandt auf das Standardskalarprodukt in \(\mathbb{R}^2\) oder \(\mathbb{R}^3\) kann man alle üblichen Sätze der euklidischen Geometrie beweisen, z.B. Sinussatz, Kosinussatz, Fasskreisbogen,\dots
\subsubsection{Definition}
Sei \((V,\Phi)\) ein euklidischer Vektorraum.
(a) Eine Menge \(M\) von Vektoren aus \(V\) heißt \ul{paarweise orthogonal}, wenn für \(v,w\in M\) mit \(v\neq w\) gilt \(\Phi(v,w)=0\).
(b) Eine paarweise orthogonale Menge \(M\subseteq V\) heißt \ul{orthonormal}, wenn für \(v\in M\) gilt \(\norm{v}_\Phi=1\).
(c) Eine Basis \(B\) von \(V\) heißt \ul{Orthogonalbasis} (OGB), wenn sie aus paarweise orthogonalen Vektoren besteht.
(d) Eine OGB von \(V\) heißt eine \ul{Orthonormalbasis} (ONB) von \(V\), wenn die Vektoren orthonormal sind.
\subsubsection{Bemerkung}
Ist \(B=\{v_1,\dots,v_n\}\) eine OGB von \(V\), so ist \(\tilde{B}=\{\frac{v_1}{\norm{v_1}_\Phi},\dots,\frac{v_n}{\norm{v_n}_\Phi}\}\) eine ONB von \(V\). Der Übergang von \(B\) zu \(\tilde{B}\) heißt \ul{Normieren}.
\subsubsection{Satz (Das Schmidtsche Orthogonalisierungsverfahren)}
Sei \((V,\Phi)\) ein euklidischer Vektorraum und \(B=\{v_1,\dots,v_n\}\) eine Basis von \(V\). Die folgenden Instruktionen definieren einen Algorithmus, der eine ONB \(C\) von \(V\) berechnet:\\
(1) Setze \(w_1=\frac{v_1}{\norm{v_1}}\)
(2) Für \(i=2,3,\dots,n\) berechne der Reihe nach \(\tilde{v_i}=\Phi(v_i,w_1)\cdot w_1+\dots+\Phi(v_i,w_{i-1})\cdot w_{i-1}\) und \(w_i=\frac{v_i-\tilde{v_i}}{\norm{v_i-\tilde{v_i}}}\)
(3) Gib \(C=\{w_1,\dots,w_n\}\) aus und stoppe.\\
\proof
\ul{Endlichkeit:} klar\\
\ul{Korrektheit:} Wir schließen mit Induktion nach i und zeigen, dass \(\{w_1,\dots,w_i\}\) orthonormal ist.\\
\ul{\(i=1\):} \(\norm{w_1}=\norm{\frac{v_1}{\norm{v_1}}}=\frac{\norm{v_1}}{\norm{v_1}}=1\).\\
\ul{\(i>1\):} Nach Konstruktion gilt \(w_i=\frac{v_i-\tilde{v_i}}{\norm{v_i-\tilde{v_i}}}\in  <v_i,w_1,\dots,w_{i-1}>\underset{\text{Induktion}}{=}<v_1,\dots,v_i>\) und es gilt \(v_i\notin <v_1,\dots,v_{i-1}>=<w_1,\dots,w_{i-1}>\). Also folgt \(v_i\neq \tilde{v_i}\) und somit \(\norm{v_i-\tilde{v_i}}\neq 0\).\\
	Für \(j=1,\dots,i-1\) gilt\\
	\(\Phi(w_j,w_i)=\frac{1}{\norm{v_i-\tilde{v_i}}}\Phi(v_i-\tilde{v_i},w_j)=\frac{1}{\norm{v_i-\tilde{v_i}}}\cdot \left[\Phi(v_i,w_j)-\Phi(\Phi(v_i,w_1)\cdot w_1,w_j)-\dots-\Phi(\Phi(v_i,w_{i-1})w_{i-1},w_j)\right]\)\\
	\(\frac{1}{\norm{v_i-\tilde{v_i}}}\left[\Phi(v_i,w_j)-\Phi(v_i,w_1)\cdot\underbrace{\Phi(w_1,w_j)}_{=0}-\dots-\Phi(w_i,w_{i-1})\cdot \underbrace{\Phi(w_{i-1},w_j)}_{=0}\right]=\frac{1}{\norm{v_i-\tilde{v_i}}}\cdot\left[\Phi(v_i,w_j)-\Phi(v_i,w_j)\cdot 1\right]=0\).\\
	Somit folgt \(w_i\bot <w_1,\dots,w_{i-1}>\) und offenbar gilt \(\norm{w_i}=1\).
	\qed
	\subsubsection{Beispiel}
	Sei \(V=\mathbb{R}^2\) und \(\Phi\) das Standardskalarprodukt. Ferner sei \(B=\{v_1,v_2\}\) mit \(v_1=(1,1)\) und \(v_2=(0,3)\).
	(1) \(w_1=\frac{v_1}{\norm{v_1}}=(\frac{1}{sqrt{2}},\frac{1}{sqrt{2}})\).
	(2) \(\tilde{v_2}=\Phi(v_2,w_1)\cdot w_1=\Phi((0,3),(\frac{1}{sqrt{2}},\frac{1}{sqrt{2}}))\cdot (\frac{1}{sqrt{2}},\frac{1}{sqrt{2}})=(\frac{3}{2},frac{3}{2})\).\\
	\(w_2=\frac{v_2-\tilde{v_2}}{\norm{v_2-\tilde{v_2}}}=\frac{(\frac{3}{2},\frac{3}{2})}{\norm{(\frac{3}{2},\frac{3}{2})}}=\frac{(-\frac{3}{2},\frac{3}{2})}{\frac{3}{\sqrt{2}}}=(-\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2})\).\\
	Also ist \(\tilde{B}=\{(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}),(-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})\}\) eine ONB von \(\mathbb{R}^2\).
	\subsubsection{Korollar}
	Ist \(M=\{v_1,\dots,v_k\}\) eine orthogonale Menge von Vektoren in \(V\), so kann man \(M\) zu einer ONB \(B=\{v_1,\dots,v_k,v_{k+1},\dots,v_n\}\) von \(V\) ergänzen.\\
	\proof
	Ergänze \(\{v_1,\dots,v_k\}\) durch \(\{w_{k+1},\dots,w_n\}\) zu einer Basis von \(V\) und wende dann Satz 26.11 an.
	\qed
	\subsubsection{Definition}
	Sei \((V,\Phi)\) ein euklidischer Vektorraum und seien \(U,W\) zwei Untervektorräume von \(V\). Gilt \(V=U\oplus W\) und \(U\bot_\Phi W\), so heißt \(V\) die \ul{orthogonale direkte Summe} von \(U\) und \(W\).\\
	\ul{Schreibweise:} \(V=U\oplus W\) TODOOPLUS NUR OBEN STRICh
	\subsubsection{Satz}
	Seu \((V,\Phi)\) ein euklidischer Vektorraum und \(U\subseteq V\) ein Untervektorraum.\\
	Dann gilt \(V=U\obot U^\bot\) und für alle \(W\subseteq V\) mit \(V=U\oplus W\) gilt \(W=U^\bot\). In diesem Fall heißt \(U^\bot\) das \ul{orthogonale Komplement} von \(U\).\\
	\proof
	Wegen der Dimensionsformel ist nur \(U\cap U^\bot=\{0\}\) zu zeigen. Für \(v\in U\cap U^\bot\) gilt \(\Phi(v,v)=0\Rightarrow v=0\).
	\qed
	\subsubsection{Bemerkung}
	(a) In \(V=\mathbb{R}^2\) betrachte \(v=(a_1,a_2)\) und \(w=(b_1,b_2)\). Dann ist die Fläche des von \(v und w\) aufgespannten Parallelogramms gegeben durch \(Vol(v,w)=\abs{a_1b_2-a_2b_1}=\abs{det\begin{pmatrix}
		a_1 & b_1\\
		a_2 & b_2
		\end{pmatrix}}\).
	(b) In \(V=\mathbb{R}^3\) betrachte \(v=(a_1,a_2,a_3),w=(b_1,b_2,b_3),u=(c_1,c_2,c_3)\). Dann ist das Volumen des von \(v,w,u\) aufgespannten \ul{Spats} (\ul{Parallelepipeds}) gegeben durch \(Vol(v,w,u)=\abs{det\underbrace{\begin{pmatrix}
			a_1 & b_1 & c_1\\
			a_2 & b_2 & c_2\\
			a_3 & b_3 & c_3
			\end{pmatrix}}_{A}}=\abs{det(A)}\)\\
		Sei \(B=(v,w,u)\) eine Basis von \(\mathbb{R}^3\).\\
		Die Gramsche Matrix des Standardskalarprodukts bzgl. der Basis \(B\) ist \(G_B(\Phi)=A^{tr}G_\xi(\Phi)A=A^{tr}A\) und es folgt \(det(G_B(\Phi))=det(A^{tr}A)=det(A^{tr})det(A)=det(A)^2\geq 0\). Dies liefert \(Vol(v,w,u)=\sqrt{det(G_B(\Phi))}\).
		\subsubsection{Definition}
		Sei \(V\) ein reeller Vektorraum und \(\Phi:V\times V\rightarrow \mathbb{R}\) eine Bilinearform auf \(V\). Ist \(B=(v_1,\dots,v_m)\) ein Tupel von Vektoren aus \(V\) mit \(G_B(\Phi)=(\Phi(v_i,v_j))_{i,j}\in Mat_m(\mathbb{R})\), so heißt \(G(v_1,\dots,v_m)=det(G_B(\Phi))\) die \ul{Gramsche Determinante} von \(\Phi\) bzgl. \(B\).
		\subsubsection{Satz}
		Sei \((V,\Phi)\) ein euklidischer Vektorraum und \(B=(v_1,\dots,v_n)\) ein Tupel von Vektoren aus \(V\).
		(a) Die Gramsche Determinante erfüllt \(G(v_1,\dots,v_n)=det(\Phi(v_i,v_j))_{i,j}\geq 0\).
		(b) Es gilt: \(G(v_1,\dots,v_n)=0\Leftrightarrow \{v_1,\dots,v_n\}\) linear abhängig.\\
		\proof
		"(a)" Sei \(C=(w_1,\dots,w_n)\) eine ONB von \(V\). Für \(i=1,\dots,m\) schreibe \(v_i=a_{i1}w_1+\dots+a_{im}w_m\) mit \(a_{ij}\in \mathbb{R}\).
		Setze \(A=(a_{ij})\in Mat_{m,n}(\mathbb{R})\). Betrachte den \ul{Fall \(m\leq n\):} Wähle Vektoren \(v_{m+1},\dots,v_n\) von \(V\), die Teil einer ONB von \(<v_1,\dots,v_n>^\bot\) sind.\\
Betrachte \(\tilde{A}=\begin{pmatrix}
A\\e_{m+1}\\\vdots\\e_n
\end{pmatrix}=TODO MATRICES\in Mat_n(\mathbb{R})\) und erhalte \(\tilde{A}\tilde{A}^{tr}=TODO MATRICES\) mit \(A'=A_2,A''=A_2^{tr}\) für \(A=\underset{\text{quadratisch}}{(A_1|A_2)}\). Es folgt: \(det(AA^{tr})=det(\tilde{A}\tilde{A}^{tr})=det(\tilde{A})\cdot det(\tilde{A}^{tr})=det(\tilde{A})^2\geq 0\). Beachte \(\Phi(v_i,v_j)=0\) für \(i\leq i\leq n\). Dies liefert \(A'=0\) und damit \circled{\(*\)}. Nun folgt die Behauptung aus \(G(v_1,\dots,v_m)=det\begin{pmatrix}
\Phi(v_1,v_1),\dots,\Phi(v_1,v_m)\\
\vdots\\
\Phi(v_m,v_1),\dots,\Phi(v_m,v_n)
\end{pmatrix}=det(AA^{tr})\geq 0\).
"(b)" Sind \(v_1,\dots,v_m\) linear abhängig, so sind die Zeilen von \(A\), bzw. \(\tilde{A}\) linear abhängig und es folgt \(G(v_1,\dots,v_n)=det(\tilde{A})^2=0\). Umgekehrt gilt:\\
\(G(v_1,\dots,v_m)=0\Rightarrow det(\tilde{A})^2=0\Rightarrow det(\tilde{A})=0\Rightarrow \{v_1,\dots,v_m\}\) linear abhängig.
\qed
\subsubsection{Definition}
Sei \((V,\Phi)\) ein euklidischer Vektorraum und \(B=(v_1,\dots,v_n)\) ein Basistupel von \(V\). Dann heißt \(Vol(v_1,\dots,v_n)=\sqrt{G(v_1,\dots,v_n)}\) das \ul{n-dimensionale Volumen} des von \(\{v_1,\dots,v_n\}\) aufgespannten Parallelepipeds bzgl. \(\Phi\).
\subsubsection{Satz (Die Ungleichung von Hadamard)}
Sei \(B=(v_1,\dots,v_n)\) ein Tupel von Vektoren in \(V\). Dann gilt: \(Vol(v_1,\dots,v_n)\leq \norm{v_1}_\Phi\cdot\dots\cdot\norm{v_n}_\Phi\) mit Gleichheit genau dann, wenn \(B\) eine Orthogonalbasis von \(V\) ist.\\
\proof
Ist \(B\) linear abhängig, so gilt \(Vol(v_1,\dots,v_n)=0\) und die Beh. ist klar.\\
Wegen Sart 26.18 darf man quadrieren und es genügt zu zeigen: \(G(v_1,\dots,v_n)\leq \Phi(v_1,v_1)\cdot\dots\cdot\Phi(v_n,v_n)\).\\
Mit Induktion zeigen wir, dass es für \(i=1,\dots,n\) eine eindeutig bestimmte Zerlegung \(v_i=v_i'+v_i''\) gibt mit \(v_i'\in<v_1,\dots,v_{i-1}>_\mathbb{R}\) und \(v_i''\bot_\Phi<v_1,\dots,v_{i-1}>_\mathbb{R}\) und dann gilt: \(G(v_1,\dots,v_i)\underset{(1)}{=}G(v_1,\dots,v_{i-1})\cdot\Phi(v_i'',v_i'')\underset{(2)}{\leq}G(v_1,\dots,v_{i-1})\cdot\Phi(v_i,v_i)\). Für \(i=n\) ergibt sich dann die Beh. des Satzes.\\
\ul{\(i=1\):} klar\\
\ul{\(i-1\rightarrow i\):} Die Existenz und Eindeutigkeit der Zerlegung \(v_i=v_i'+v_i''\) wurde im Beweis des Schmidtschen Orthogonalisierungsverfahrens gezeigt.\\
"(2)" Es gilt: \(\Phi(v_i,v_i)=\Phi(v_i'+v_i'',v_i'+v_i'')=\Phi(v_i',v_i')+\Phi(v_i'',v_i'')\geq\Phi(v_i'',v_i'')\), da \(\Phi(v_i',v_i'')=0\).\\
"(1)" Verwende \(\Phi(v_j,v_i)=\Phi(v_j,v_i')\) für \(j<i\) und \(\Phi(v_i,v_i)=\Phi(v_i',v_i')+\Phi(v_i'',v_i'')\). Es folgt:\\
\(G(v_1,\dots,v_i)=det\begin{pmatrix}
\Phi(v_1,v_1) & \hdots & \Phi(v_1,v_{i-1}) & \Phi(v_1,v_i')\\
\vdots & & \vdots & \vdots\\
\Phi(v_{i-1},v_1) & \hdots & \Phi(v_{i-1},v_{i-1}) &\Phi(v_{i-1},v_i') \\
\Phi(v_i',v_1) & \hdots & \Phi(v_{i-1}',v_{i-1}) & \Phi(v_i',v_i')+\Phi(v_i'',v_i'')
\end{pmatrix}\).TODO Linien vertikal horizontal\\
Schreibe \(v_i'=c_1v_1+\dots+c_{i-1}v_{i-1}\) mit \(c_k\in\mathbb{R}\) und subtrahiere für \(j=1,\dots,i-1\) das \(c_j\)-fache der \(j\)-ten Spalte von der letzten Spalte. Die neue letzte Spalte ist dann \(\begin{pmatrix}
0\\
\vdots\\
0\\
\Phi(v_i'',v_i'')
\end{pmatrix}\). 
Entwicklung der Determinante nach der letzten Spalte und die Definition von \(G(v_1,\dots,v_{i-1})\) liefern dann die Behauptung.
\qed
\subsection{Die Hierarchie der reellen Vektorräume}
Sei \(V\) ein \(n\)-dimensionaler \(\mathbb{R}\)-Vektorraum.
\subsubsection{Bemerkung}
(Wdh.) Ist \((V,\Phi)\) ein euklidischer Vektorraum, so ist \((V,\norm{\cdot}_\Phi)\) ein normierter Vektorraum bzgl. \(\norm{v}_\Phi=\sqrt{\Phi(v,v)}\)
\subsubsection{Satz}
(a) (Die Parallelogramm-Gleichung) Ist \(\Phi\) ein Skalarprodukt auf \(V\), so gilt für alle \(v,w\in V\): \(\norm{v+w}_\Phi^2+\norm{v-w}_\Phi^2=2\norm{v}_\Phi^2+2\norm{w}_\Phi^2\).
(b) Ist \((V,\norm{\cdot})\) ein normierter Vektorraum und erfüllt \(\norm{\cdot}\) die Parallelogrammgleichung, so existiert ein Skalarprodukt \(\Phi\) auf \(V\) mit \(\norm{v}=\norm{v}_\Phi=\sqrt{\Phi(v,v)}\) für alle \(v\in V\).\\
\proof
Vgl. Übungen
\subsubsection{Definition}
(a) Eine \ul{Metrik} \(d\) auf \(V\) ist eine Abbildung \(d:V\times V\rightarrow \mathbb{R}\) mit folgenden Eigenschaften:
(1) Für \(v,w\in V\) gilt \(d(v,w)\geq 0\) und \(d(v,w)=0\Leftrightarrow v=w\).
(2) Für \(v,w\in V\) gilt \(d(v,w)=d(w,v)\)
(3) (Dreiecksungleichung) Für \(v,w,u\in V\) gilt \(d(v,w)\leq d(v,u)+d(u,w)\)
(b) Existiert eine Metrik \(d\) auf \(V\), so heißt \((V,d)\) ein \ul{metrischer Vektorraum}.
\subsubsection{Beispiel}
Sei \(K=\mathbb{F}_2\) und \(V=\mathbb{F}_2^n\). Definiere für \(v=(a_1,\dots,a_n)\) und \(w=(b_1,\dots,b_n)\) mit \(a_i,b_j\in\{0,1\}\) die Zahl \(d(v,w)=\#\{i\in\{1,\dots,n\}|a_i\neq b_i\}\). Dann ist \(d:V\times V\rightarrow \mathbb{R}\) eine Metrik (Beweis: Übungen). Sie heißt die \ul{Hamming-Metrik} (oder der \ul{Hamming-Abstand}) auf \(V\).
\subsubsection{Bemerkung}
Ist \((V,\norm{\cdot})\) ein normierter Vektorraum, so wird durch folgende Abbildung \(d:\underset{(v,w)\mapsto \norm{v-w}}{V\times V\rightarrow \mathbb{R}}\) eine Metrik auf \(V\) definiert. Also ist jeder normierte Vektorraum ein metrischer Vektorraum.
\subsubsection{Definition}
Sei \(\mathfrak{U}=\{U_i|i\in I\}\) eine Menge von Teilmengen von \(V\) mit folgenden Eigenschaften:
(1) \(\varnothing\in \mathfrak{U},V\in\mathfrak{U}\)
(2) \(U_1,\dots,U_n\in\mathfrak{U}\Rightarrow U_1\cap\dots\cap U_n\in\mathfrak{U}\) (d.h. \(\mathfrak{U}\) ist abgeschlossen bzgl. endlichen Durchschnitten)
(3) Ist \(J\subseteq I\), so gilt \(\underset{j\in J}{\bigcup}U_j\in\mathfrak{U}\) (d.h. \(\mathfrak{U}\) ist abgeschlossen bzgl. beliebigen Vereinigungen).\\
Dann heißt \(\mathfrak{U}\) eine \ul{Topologie} auf \(V\) und \((V,\mathfrak{U})\) heißt ein \ul{topologischer Vektorraum}. Die Mengen \(U\in\mathfrak{U}\) heißen \ul{offen} und die Mengen der Form \(V\backslash U\) mit \(U\in\mathfrak{U}\) heißen \ul{abgeschlossen}.
\subsubsection{Bemerkung}
(a) Ist \((V,d)\) ein metrischer Vektorraum, so definieren wir für jedes \(V\in V\) und \(\epsilon\in\mathbb{R}_+\) die \ul{offene \(\epsilon\)-Kugel um v} durch \(B_\epsilon(v)=\{w\in V|d(v,w)<\epsilon\}\)
(b) Eine Teilmenge \(U\subseteq V\) heiße \ul{offen}, wenn sie eine Vereinigung von \(\epsilon\)-Kugeln ist: \(U=\underset{i\in I}{\bigcup}B_{\epsilon_i}(v_i)\)
(c) Die Menge aller dieser offenen Teilmengen von \(V\) bildet eine Topologie auf \(V\). Somit ist jeder normierte Vektorraum ein topologischer Vektorraum.\\
\begin{center}
	\ul{Die Hierarchie reeller Vektorräume:}\\
	\(V\) euklidischer Vektorraum\\
	\(\Downarrow\)    \sout{\(\Uparrow\)}\\
	\(V\) normierter Vektorraum\\
	\(\Downarrow\)    \sout{\(\Uparrow\)}\\
	\(V\) metrischer Vektorraum\\
	\(\Downarrow\)    \sout{\(\Uparrow\)}\\
	\(V\) topologischer Vektorraum
\end{center}
\subsection{Orthogonale Endomorphismen (Buch §5.5)}
Sei \(V\) ein endlich-dimensionaler reeller Vektorraum mit Skalarprodukt \(\Phi:V\times V\rightarrow \mathbb{R}\).
\subsubsection{Definition}
(a) Eine \(\mathbb{R}\)-lineare Abbildung \(\phi:V\rightarrow V\) heißt \ul{orthogonal}, wenn für alle \(v,w\in V\) gilt: \(\Phi(\phi(v),\phi(w))=\Phi(v,w)\).
(b) Eine Abbildung \(\psi:V\rightarrow V\) heißt \ul{längentreu}, wenn für alle \(v,w\in V\) gilt: \(\norm{\psi(v)-\psi(w)}_\Phi=\norm{v-w}_\Phi\)
(c) Eine Abbildung \(\psi:V\rightarrow V\) heißt \ul{winkeltreu}, wenn für alle \(v,w\in V\backslash\{0\}\) gilt: \(\psi(v),\psi(w)\neq 0\) und \(\sphericalangle(\psi(v),\psi(w))=\sphericalangle(v,w)\)
\ul{Wdh.:} \(\sphericalangle(v,w)=arccos\left(\frac{\Phi(v,w)}{\norm{v}_\Phi\cdot\norm{w}_\Phi}\right)\)
\subsubsection{Bemerkung}
Eine orthogonale \(\mathbb{R}\)-lineare Abbildung \(\phi:V\rightarrow V\) ist längen- und winkeltreu:
(a) \ul{Längentreue:} Für \(v,w\in V\) gilt \(\norm{\phi(v)-\phi(w)}_\Phi=\sqrt{\Phi(\phi(v-w),\phi(v-w))}=\sqrt{\Phi(v-w,v-w)}=\norm{v-w}_\Phi\).
(b) \ul{Winkeltreue:} Für \(v,w\in V\backslash\{0\}\) mit \(\phi(v),\phi(w)\neq 0\) gilt \(\sphericalangle(\phi(v),\phi(w))=arccos\left(\frac{\Phi(\phi(v),\phi(w))}{\norm{\phi(v)}_\Phi\cdot\norm{\phi(w)}_\Phi}\right)=arccos\left(\frac{\Phi(v,w)}{\norm{v}_\Phi\cdot\norm{w}_\Phi}\right)=\sphericalangle(v,w)\).\\
\ul{Frage:} Wie kann man orthogonale lineare Abbildungen an ihrer Matrix erkennen?
\subsubsection{Lemma}
Sei \(B=(v_1,\dots,v_n)\) ein Basistupel von \(V\). Genau dann ist \(\phi\in End_\mathbb{R}(V)\) orthogonal, wenn für \(1\leq i,j\leq n\) mit \(i,j\in\{1,\dots,n\}\) gilt: \(\Phi(\phi(v_i),\phi(v_j))=\Phi(v_i,v_j)\).\\
\proof
"\(\Rightarrow\)" klar\\
"\(\Leftarrow\)" Zu gegebenen \(v,w\in V\) schreibe \(v=a_1v_1+\dots+a_nv_n\) und \(w=b_1v_1+\dots+b_nv_n\) mit \(a_i,b_j\in \mathbb{R}\).\\
Dann gilt: \(\Phi(\phi(v), \phi(w))=\Phi(\sum_{i=1}^na_i\phi(v_i),\sum_{j=i}^nb_j\phi(v_j))=\sum_{i=1}^n\sum_{j=i}^na_ib_j\cdot\Phi(v_i,v_j)=\Phi(\sum_{i=1}^na_iv_i,\sum_{j=i}^bb_jv_j)=\Phi(v,w)\)
\qed
\subsubsection{Satz}
Sei \(B=(v_1,\dots,v_n)\) eine ONB von \(V\).\\
Ein Endomorphismus \(\phi\in End_\mathbb{R}(V)\) ist genau dann orthogonal, wenn \(M_B(\phi)\) invertierbar ist und \(M_B(\phi)^{-1}=M_B(\phi)^{tr}\) gilt.\\
Eine solche Matrix heißt orthogonal.\\
\proof
Sei \(M_B(\phi)=(a_{ij})\in Mat_n(\mathbb{R})\). Für \(j=1,\dots,n\) gilt also \(\phi(v_j)=a_{1j}v_1+\dots+a_{nj}v_n\). Nach dem Lemma gilt: \(\phi\) orthogonal \(\Leftrightarrow\) \(\Phi(\phi(v_i),\phi(v_j))=\Phi(v_i,v_j)\) für \(1\leq i,j\leq n\). Da \(B\) eine ONB ist, gilt \(\Phi(v_i,v_j)=\delta_{ij}=\begin{cases}
1\text{ falls } i=j\\
0\text{ sonst}
\end{cases}\) (Kronecker's Delta).\\
Also gilt: \(\phi\) orthogonal \(\Leftrightarrow\) \(\Phi(a_{1i}v_1+\dots+a_{ni}v_n,a_{1j}v_1+\dots+a_{nj}v_n)=\delta_{ij}\) für \(1\leq i,j\leq n\)\\
\(\Leftrightarrow\) \(\sum_{k=1}^n\sum_{l=1}^na_{ki}a_{lj}\cdot\Phi(v_k,v_l)=\delta_{ij}\) für \(1\leq i,j\leq n\)\\
\(\Leftrightarrow\) \(\sum_{k=1}^na_{ki}a_{kj}=\delta_{ij}\) für \(1\leq i,j\leq n\) \(\Leftrightarrow\) \((a_{1i},\dots,a_{ni})\cdot\begin{pmatrix}
a_{1j}\\
\vdots\\
a_{nj}
\end{pmatrix}=\delta_{ij}\) für \(1\leq i,j\leq n\)\\
\(\Leftrightarrow\) \(e_i\cdot M_B(\phi)^{tr}\cdot M_B(\phi)\cdot e_j^{tr}=\delta_{ij}\) für \(1\leq i,j\leq n\)\\
\(\Leftrightarrow\) \(M_B(\phi)^{tr}\cdot M_B(\phi)=I_n\)\\
\(\Leftrightarrow\) \(M_B(\phi)\) invertierbar und \(M_B(\phi)^{-1}=M_B(\phi)^{tr}\)
\qed
\subsubsection{Beispiel (Orthogonale Endomorphismen von \(\mathbb{R}^2\))}
Sei \(V=\mathbb{R}^2\) und \(\Phi(v,w)=<v,w>\) das Standardskalarprodukt. Sei \(\phi:\mathbb{R}^2\rightarrow\mathbb{R}^2\) eine \(\mathbb{R}\)-lineare Abbildung und \(M_\xi(\phi)=\begin{pmatrix}
a & b\\
c & d
\end{pmatrix}\) mit \(a,b,c,d\in\mathbb{R}\).\\
\(\phi\) orthogonal \(\Leftrightarrow\) \(\begin{pmatrix}
a & b\\
c & d
\end{pmatrix}\cdot\begin{pmatrix}
a & c\\
b & d
\end{pmatrix}=\begin{pmatrix}
1 & 0\\
0 & 1
\end{pmatrix}\) \(\Leftrightarrow\) \(\begin{cases}
(1)\text{ }a^2+b^2=1\\
(2)\text{ }ac+bd=0\\
(3)\text{ }c^2+d^2=1
\end{cases}\)\\
Schreibe \(a=cos(\alpha)\), \(b=sin(\alpha)\), \(c=cos(\beta)\), \(d=sin(\beta)\) mit \(\alpha, \beta\in [0,2\pi[\). Es folgt: \(cos(\alpha)cos(\beta)+sin(\alpha)sin(\beta)=0\) \(\Leftrightarrow\) \(cos(\alpha-\beta)=0\)\\
Dies ist äquivalent mit \(\beta=\alpha\pm\frac{\pi}{2}\) oder \(\alpha=\beta\pm\frac{3\pi}{2}\) und \(M_\xi(\phi)=\begin{pmatrix}
cos(\alpha) & sin(\alpha)\\
\pm sin(\alpha) & \mp cos(\alpha)
\end{pmatrix}\)\\
\ul{1. Fall:} \(M_\xi(\phi)=\begin{pmatrix}
cos(\alpha) & sin(\alpha)\\
sin(\alpha) & -cos(\alpha)
\end{pmatrix}\): Spiegelung an der Geraden im Winkel \(\frac{\alpha}{2}\) zur x-Achse.\\
\ul{1. Fall:} \(M_\xi(\phi)=\begin{pmatrix}
cos(\alpha) & sin(\alpha)\\
-sin(\alpha) & cos(\alpha)
\end{pmatrix}\): Drehung um den Winkel \(-\alpha\) um \(0\).\\
Im ersten Fall gilt \(\chi_\phi(x)=x^2-1=(x-1)(x+1)=\mu_\phi(x)\), EW \(1,-1\), \(det(\phi)=-1\).\\
Im zweiten Fall gilt \(\chi_\phi(x)=x^2+1=\mu_\phi(x)\), keine EW, \(det(\phi)=1\).
\subsubsection{Satz (Eigenschaften orthogonaler Endomorphismen)}
Sei \((V,\Phi)\) ein \(n\)-dimensionaler Vektorraum und \(\phi\in End_\mathbb{R}(V)\) orthogonal.\\
(a) \(det(\phi)\in \{1,-1\}\)
(b) Alle Eigenwerte von \(\phi\) sind in \(\{1,-1\}\) enthalten.
(c) \(Eig(\phi,1)\bot_\Phi Eig(\phi,-1)\)
(d) Auch \(\phi^{-1}\) ist eine orthogonale \(\mathbb{R}\)-lineare Abbildung.
(e) Ist \(B\) eine ONB von \(V\), so bilden die Spalten von \(M_B(\phi)\) eine ONB von \(K^n\) bzgl. \(<,>\), und die Zeilen von \(M_B(\phi)\) ebenso.\\
\proof
"(a)" Es gilt \(M_B(\phi)\cdot M_B(\phi)^{tr}=I_n\). Dann folgt \(1=det(M_B(\phi))\cdot M_B(\phi)^{tr}=(det M_B(\phi))^2=det(\phi)^2\), also \(det(\phi)\in\{-1,1\}\).
"(b)" Sei \(\lambda\in \mathbb{R}\) ein EW von \(\phi\) und \(v\in V\backslash\{0\}\) ein EV zum EW \(\lambda\), d.h. es gelte \(\phi(v)=\lambda\cdot v\). Dann folgt \(\Phi(v,v)=\Phi(\phi(v),\phi(v))=\Phi(\lambda v, \lambda v)=\lambda^2\Phi(v,v)\). Wegen \(v\neq 0\) folgt \(\Phi(v,v)>0\) und \(\lambda^2=1\).
"(c)" Sei \(v\in Eig(\phi,1)\) und \(w\in Eig(\phi,-1)\). Dann gilt \(\phi(v,w)=\Phi(\phi(v),\phi(w))=\Phi(v,-w)=-\Phi(v,w)\) und somit \(\Phi(v,w)=0\).
"(d)" Die Matrix von \(\phi^{-1}\) ist \(M_B(\phi^{-1})=M_B(\phi)^{-1}=M_B(\phi)^{tr}\) und somit \(M_B(\phi^{-1})\cdot M_B(\phi^{-1})^{tr}=M_B(\phi)^{-1}\cdot M_B(\phi)=I_n\). Also ist \(\phi1{-1}\) orthogonal.
"(e)" Für eine ONB \(B=(v_1,\dots,v_n)\) von \(V\) gilt \(\Phi(v_i,v_j)=\delta_{ij}\) für \(1\leq i,j\leq n\) und somit \(\Phi(\phi(v_i), \phi(v_j))=\delta_{ij}\) für \(1\leq i,j\leq n\). Also ist auch \((\phi(v_1),\dots,\phi(v_n))\) eine ONB von \(V\). Schreibe \(\phi(v_j)=a_{1j}v_1+\dots+a_{nj}v_n\) mit \(a_{ij}\in\mathbb{R}\). Dann gilt: \(<\begin{pmatrix}
a_{1i}\\
\vdots\\
a_{ni}
\end{pmatrix},\begin{pmatrix}
a_{1j}\\
\vdots\\
a_{nj}
\end{pmatrix}>=a_{1i}a_{1j}+\dots+a_{ni}a_{nj}=\sum_{k=1}^n\sum_{l=1}^na_{ki}a_{lj}\delta_{kl}=\sum_{k=1}^n\sum_{l=1}^na_{ki}a_{lj}\Phi(v_k,v_l)=\Phi(\phi(v_i),\phi(v_j))=\delta_{ij}\). Die Zeilen von \(M_B(\phi)\) sind die Spalten von \(M_B(\phi^{-1})\) und die Behauptung für die Zeilen folgt aus (d).
\qed
\subsubsection{Beispiel (Orthogonale Endomorphismen des \(\mathbb{R}^3\))}
Sei \(V=\mathbb{R}^3\) und \(\phi\in End_\mathbb{R}(\mathbb{R}^3)\) orthogonal bzgl. \(<,>\).\\
Dann gilt: \(\chi_\phi(x)=-x^3+\dots\pm 1\). Wegen \(\underset{x\rightarrow-\infty}{lim}\chi(x)=-\infty\) und \(\underset{x\rightarrow\infty}{lim}\chi(x)=\infty\) besitzt \(\chi_\phi(x)\) eine Nullstelle, also \(\phi\) einen EW \(\lambda=\pm 1\). Sei \(v_1\in\mathbb{R}^3\backslash\{0\}\) ein EV zum EW \(\lambda\). Ergänze \(v_1\) zu einer ONB \(B=(v_1,v_2,v_3)\) von \(\mathbb{R}^3\). Dann gilt \(M_B(\phi)=\begin{pmatrix}
\lambda& 0 & 0\\
0 & a & b\\
0 & c & d
\end{pmatrix}\) mit \(a,b,c,d\in\mathbb{R}\). Wegen \(M_B(\phi)M_B(\phi)^{tr}=\begin{pmatrix}
\lambda^2 & 0 & 0\\
0 & a^2+b^2 & ac+bd\\
0 & ac+bd & c^2+d^2
\end{pmatrix}=I_3\) ist auch \(\begin{pmatrix}
a & b\\
c & d
\end{pmatrix}\) eine orthogonale Matrix, entsprechend einer Spiegelung oder Drehung in \(\mathbb{R}^2\).\\
\ul{1. Fall:} \(\lambda=-1\), \(det\begin{pmatrix}
a & b\\
c & d
\end{pmatrix}=-1\), d.h. \(\begin{pmatrix}
a & b\\
c & d
\end{pmatrix}\) entspricht einer Spiegelung. Dann gibt es eine ONB \(w_1,w_2\) von \(<v_2,v_3>_\mathbb{R}\) mit \(\phi(w_1)=w_1\) und \(\phi(w_2)=-w_2\).
Die Matrix von \(\phi\) bzgl. der ONB \(C=(v_1,w_2,w_3)\) ist \(M_C(\phi)=\begin{pmatrix}
-1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & -1
\end{pmatrix}\). Also ist \(\phi\) eine Drehung um die y-Achse um 180°.\\
\ul{2. Fall:} \(\lambda=1\), \(det\begin{pmatrix}
a & b\\
c & d
\end{pmatrix}=-1\). Wähle \((w_2,w_3)\) wie eben und erhalte für \(D=(v_1,w_2,w_3)\) die Matrix \(M_D(\phi)=\begin{pmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & -1
\end{pmatrix}\). Dies ist die Spiegelung an der (x,y)-Ebene.\\
\ul{3. Fall:} \(\lambda=-1\), \(det\begin{pmatrix}
a & b\\
c & d
\end{pmatrix}=1\) , d.h. \(\begin{pmatrix}
a & b\\
c & d
\end{pmatrix}\) definiert eine Drehung und es existiert \(\alpha\in[0,2\pi[\) mit \(\begin{pmatrix}
a & b\\
c & d
\end{pmatrix}=\begin{pmatrix}
cos(\alpha) & -sin(\alpha)\\
sin(\alpha) & cos(\alpha)
\end{pmatrix}\). Bzgl. der entsprechenden ONB \(E\) gilt \(M_E(\phi)=\begin{pmatrix}
-1 & 0 & 0\\
0 & cos(\alpha) & -sin(\alpha)\\
0 & sin(\alpha) & cos(\alpha)
\end{pmatrix}\). Dies ist eine Drehspiegelung mit Drehachse \(\hat{=}\) x-Achse, Drehwinkel \(\hat{=}\) \(\alpha\), Spiegelebene \(\hat{=}\) (y,z)-Ebene\\
\ul{4. Fall:} \(\lambda=1\), \(det\begin{pmatrix}
a & b\\
c & d
\end{pmatrix}=1\) \(\rightarrow\) \(M_F(\phi)=\begin{pmatrix}
1 & 0 & 0\\
0 & cos(\alpha) & -sin(\alpha)\\
0 & sin(\alpha) & cos(\alpha)
\end{pmatrix}\) Drehung um die x-Achse um den Winkel \(\alpha\).
\subsubsection{Definition (Die orthogonale Gruppe)}
(a) Die bzgl. \(\Phi\) orthogonalen Endomorphismen von \(V\) bilden eine Gruppe \(O_\Phi(V)\), die wir die (bzgl. \(\Phi\)) orthogonale Gruppe von \(V\) nennen.
(b) \(O_n(\mathbb{R})=\{A\in Mat_n(\mathbb{R})|A\text{ orthogonal}\}\) heißt die \ul{orthogonale Gruppe} (\(n\)-ter Stufe).
(c) \(SO_N(\mathbb{R})=\{A\in O_n(\mathbb{R})|det(A)=1\}\) heißt die \ul{spezielle orthogonale Gruppe} (\(n\)-ter Stufe).
(d) Die Elemente von \(SO_n(\mathbb{R})\) heißen auch \ul{Drehungen}.\\
Die Elemente von \(O_n(\mathbb{R})\backslash SO_n(\mathbb{R})\) heißen \ul{Drehspiegelungen}.
\subsubsection{Lemma}
Sei \(f\in \mathbb{R}\left[x\right]\backslash\mathbb{R}\). Dann ist \(f\) in (bis auf die Reihenfolge und skalare Faktoren) eindeutiger Weise ein Produkt aus linearen und irreduziblen quadratischen Polynomen, d.h. \(f=c\cdot l_1^{\alpha_1}\cdot\dots\cdot l_r^{\alpha_r}q_1^{\beta_1}\cdot\dots\cdot q_s^{\beta_s}\) mit \(c\in\mathbb{R}\), \(l_i\) normiertes lineares Polynom, \(\alpha_i\geq 1\), \(q_j\) normiertes irreduzibles Polynom vom Grad 2, \(\beta\geq 1\).\\
\proof
\begin{itemize}
\item Fundamentalsatz der Algebra
\item komplexe Konjugation liefert, dass die komplexen Nullstellen in Paaren \(a_k+ib_k\) auftreten: \((x-(a_k+ib_k))(x-(a_k-ib_k))\in\mathbb{R}\left[x\right]\).
\end{itemize}
\qed
\subsubsection{Satz (Normalformen orthogonaler Endomorphismen)}
Zu jeder orthogonalen \(\mathbb{R}\)-linearen Abbildung \(\phi:V\rightarrow V\) gibt es eine ONB \(B\) von \(V\) mit \(M_B(\phi)=TODO MATRIX\) mit \ul{Drehkästchen} \(Q_i=\begin{pmatrix}
cos(\alpha_i) & -sin(\alpha_i)\\
sin(\alpha_i) & cos(\alpha_i)
\end{pmatrix}\), \(\alpha_i\in \left[ 0,2\phi\right[\).\\
\proof
Wir schließen mit vollständiger Induktion nach \(n=dim_\mathbb{R}(V)\).\\
\ul{\(n=1\):} Klar, da \(M_B(\phi)=(\pm 1)\)\\
\ul{\(n=2\):} vgl. Beispiel 28.5\\
\ul{\(n>2\):} Schreibe \(\chi_\phi(x)=(-1)^nl_1^{\alpha_1}\cdot\dots\cdot l_s^{\alpha_s}q_1^{\beta_1}\cdot\dots\cdot q_t^{\beta_t}\) mit \(l_i\) linear, \(\alpha_i\geq 1\), \(q_j\) quadratisch, \(\beta_j\geq 1\). Im Fall \(s\geq 1\) sei \(U=Gen(\phi, l_1)\) und im Fall \(s=0, t\geq 1\) sei \(U=Gen(\phi,q_1)\).\\
Da \(\phi\) mindestens zwei EF besitzt, gilt \(U\subsetneq V\). Ferner ist \(U\) \(\phi\)-invariant, d.h. \(\phi(U)\subseteq U\). Verwende nun \(V=U\obot U^\bot\).\\
Für \(v\in U^\bot\) und \(w\in U\) gilt: \(\Phi(\phi(v), w)=\Phi(\phi^{-1}\phi(v), \phi^{-1}(w))=\Phi(v, \underbrace{\phi^{-1}(w)}_{\in U})=0\) und somit \(\phi(v)\in U^\bot\).\\
Also gilt auch \(\phi(U^\bot)\subseteq U^\bot\).\\
Nun konstruieren wir nach Induktionsvoraussetzung zwei ONB \(B_1\) von \(U\) und \(B_2\) von \(U^\bot\) bzgl. denen \(M_{B_1}(\phi)_U\) (bzw. \(M_{B_2}(\phi)_{U^\bot}\)) die gewünschte Gestalt hat. Dann ist \(B_1\cup B_2\) eine ONB von \(V\) bzgl. der (evtl. nach Umordnung)  die Matrix \(M_{B_1\cup B_2}(\phi)\) die gesuchte Gestalt hat.
\qed
\subsubsection{Definition}
Sei \(V\) ein \(n\)-dimensionaler \(\mathbb{R}\)-Vektorraum und \(\phi:V\rightarrow V\) ein Automorphismus.
Die Abbildung \(\phi\) heißt \ul{orientierungserhaltend} (oder \ul{orientierungstreu}), wenn \(det(\phi)>0\) gilt. Ansonsten heißte \(\phi\) \ul{orientierungsumkehrend}.
\subsubsection{Beispiel}
(a) Sei \(\phi:\mathbb{R}^2\rightarrow \mathbb{R}^2\) mit \(M_\xi(\phi)=\begin{pmatrix}
-1 & -\frac{1}{4}\\
1 & -\frac{1}{2}
\end{pmatrix}\). Dann gilt \(det(\phi)=\frac{3}{4}\). Die Abbildung \(\phi\) ist orientierungserhaltend.
(b) Betrachte \(\phi:\mathbb{R}^2\rightarrow \mathbb{R}^2\) mit \(M_\xi(\psi)=\begin{pmatrix}
-\frac{1}{4} & 1\\
1 & \frac{1}{2}
\end{pmatrix}\), \(det(\psi)=\frac{-9}{8}\).
\subsubsection{Bemerkung}
Für \(\phi\in End_\mathbb{R}(\mathbb{R}^n)\) gilt:\\
\(\abs{det(\phi)}=\abs{det(\phi(e_1)\phi(e_2)\dots\phi(e_n))}\). Also misst \(\abs{det(\phi)}\) das Volumen des Bilds des Einheitswürfels. Also misst \(\abs{det(\phi)}\) die Volumenänderung unter \(\phi\).
\subsubsection{Definition}
Seien \(B=(v_1,\dots,v_n)\) und \(C=w_1, \dots, w_n\) zwei Basen von \(V\). Sei \(\phi:V\rightarrow V\) der Automorphismus mit \(\phi(v_i)=w_i\) für \(i=1,\dots,n\).
(a) Die Basen \(B\) und \(C\) heißen \ul{gleich orientiert}, wenn \(\phi\) orientierungserhaltend ist, d.h. wenn \(det(\phi)>0\) gilt. In diesem Fall schreiben wir \(B\sim_{OR} C\). Andernfalls heißen \(B\) und \(C\) entgegengesetzt orientiert.
(b) Offenbar ist \(\sim_{OR}\) eine Äquivalenzrelation. Eine \ul{Orientierung} auf \(V\) ist eine Äquivalenzklasse \(\left[B\right]\) gleichorientierter Basen.
\subsubsection{Beispiel (Orientierungen im \(\mathbb{R}^2\))}
(a) Die Äquivalenzklasse \(\left[\xi\right]\) der Standardbasis heißt der \ul{Gegenuhrzeigersinn} (oder der \ul{mathematisch positive Drehsinn})
(b) Die Äquivalenzklasse \(\left[(e_2,e_1)\right]=\left[(e_1,-e_2)\right]\) heißt der \ul{Uhrzeigersinn}
\subsubsection{Beispiel (Orientierungen im \(\mathbb{R}^3\))}
(a) Die Äquivalenzklasse \(\left[\xi\right]\) heißt \ul{Rechte-Hand-Orientierung}
(b) Die Äquivalenzklasse \(\left[(e_1,e_3,e_2)\right]\) heißt \ul{Linke-Hand-Orientierung}
\subsubsection{Satz (Der Satz vom Fußball)}
Bei einem Fußballspiel liegt der Ball am Anfang der 1. Halbzeit und am Anfang der 2. Halbzeit genau am Anstoßpunkt. Dann gibt es 2 Punkte auf der Balloberfläche, die zu beiden Zeitpunkten an exakt derselben Stelle sind.\\
\proof
Führe in \(\mathbb{R}^3\) ein kartesisches Koordinatensystem so ein, dass der Mittelpunkt des Balls \(0=(0,0,0)\) ist und sein Radius 1. Die Balloberfläche ist dann die \ul{2-Sphäre} \(S^2=\{(x,y,z)\in\mathbb{R}^3|x^2+y^2+z^2=1\}\).\\
Sei \(\tilde{\phi}:S^2\rightarrow S^2\) die Abb., die jedem Punkt auf \(S^2\) am Beginn der 1. Halbzeit den zugehörigen Punkt am Beginn der 2. Halbzeit zuordnet. Sei \(\phi:\mathbb{R}^3\rightarrow \mathbb{R}^3\) definiert durch \(\phi(v)=\begin{cases}
0 & \text{falls } v = 0\\
\norm{v}\cdot\tilde{\phi}(\frac{v}{\norm{v}}) & \text{sonst}
\end{cases}\).\\
(1) \(\phi\) erhält die Norm, denn \(\norm{\phi(v)}=\norm{\norm{v}\cdot\tilde{\phi}(\frac{v}{\norm{v}})}=\norm{v}\cdot\norm{\cdot\tilde{\phi}(\frac{v}{\norm{v}})}=\norm{v}\) für \(v\neq 0\).\\
(2) \(\tilde{\phi}\) ist winkelerhaltend.\\
(3) \(\phi\) ist winkelerhaltend, \(cos(\sphericalangle(v,w))=\frac{<v,w>}{\norm{v}\norm{w}}=\langle\frac{v}{\norm{v}},\frac{w}{\norm{w}}\rangle=\frac{<\norm{v}\tilde{\phi}(\frac{v}{\norm{v}}),\norm{w}\tilde{\phi}(\frac{w}{\norm{w}})>}{\norm{v}\cdot\norm{w}}=\frac{<\phi(v),\phi(w)>}{\norm{\phi(v)}\cdot\norm{\phi(w)}}=cos(\sphericalangle(\phi(v),\phi(w)))\)\\
(4) \(\phi(0)=0\).\\
Damit ist \(\phi\) \(\mathbb{R}\)-linear und orthogonal (\(\nearrow\) Übungen).\\
(5) Die Abb. \(\phi\) ist orientierungserhaltend. Also gilt \(det(\phi)=1\).\\
nach Bsp. 28.7 folgt, dass \(\phi\) den Eigenwert 1 besitzt und \(M_B(\phi)=TODO MATRIX\) gilt mit einem Drehkästchen Q bzgl. einer geeignet gewählten ONB \(B\) von \(\mathbb{R}^3\).\\
Die normierten Eigenvektoren zum Eigenwert 1 liefern die Behauptung.
\qed
\subsection{Symmetrische Matrizen}
\subsubsection{Theorem}
Jede symmetrische Matrix \(A\in Mat_n(\mathbb{R})\) besitzt mindestens einen reellen Eigenwert \(\lambda\in\mathbb{R}\).\\
\ul{Beweis 1:} mit Komplexifizierung\\
\ul{Beweis 2:} mit reeller Analysis\\
Betrachte die Abbildung \(q:\underset{(x_1,\dots,x_n)\mapsto (x_1,\dots,x_n)A\begin{pmatrix}
x_1\\
\vdots\\
x_n
\end{pmatrix}}{\mathbb{R}^n\rightarrow\mathbb{R}}\). Das ist eine Polynomfunktion vom Grad 2 in \(x_1,\dots,x_n\).\\
Sei \(S^{n-1}=\{(a_1,\dots,a_n)\in\mathbb{R}^n|a_1^2+\dots+a_n^2=1\}\) die \ul{\(n-1\)-Sphäre}.\\
Dann ist \(S^{n-1}\) eine kompakte (d.h. abgeschlossener und beschränkte) Teilmenge in \(\mathbb{R}^n\).\\
Jede stetige Funktion auf einem Kompaktum nimmt ein Minimum und ein Maximum an.\\
Sei \(v\in S^{n-1}\) so gewählt, dass \(q(v)\) das Maximum von \(q\) auf \(S^{n-1}\) darstellt. Ist also \(w\in S^{n-1}\) ein weiterer Einheitsvektor, so gilt \(q(v)=vAv^{tr}\geq q(w)=wAw^{tr}\).\\
\ul{Zwischenbeh.:} Ist \(w\in S^{n-1}\) mit \(v\bot w\), so gilt \(w\bot Av^{tr}\).\\
\proof
Sei \(0<\lambda<1\) und \(\mu=\sqrt{1-\lambda^2}\) und \(u=\mu v + \lambda w\). Wegen \(v\bot w\) gilt dann \(\norm{u}^2=\norm{\mu v + \lambda w} = <\mu v + \lambda w, \mu v + \lambda w>=\mu^2\norm{v}^2+\lambda^2\norm{w}^2=(1-\lambda^2)+\lambda^2=1\), also \(u\in S^{n-1}\).\\
Nach Konstruktion gilt: \(vAv^{tr}\geq uAu^{tr}=(\mu v+\lambda w)A(\mu v + \lambda w)=\mu^2vAv^{tr}+\lambda^2wAw^{tr}+\underbrace{\mu\lambda vAw^{tr}+\mu\lambda wAv^{tr}}_{\text{gleich, da } (wAv^{tr})^{tr}=vA^{tr}w^{tr}\underset{A\text{ symm.}}{=}vAw^{tr}}=\mu^2vAv^{tr}+\lambda^2wAw^{tr}+2\mu\lambda wAv^{tr}\).\\
Dann folgt \(2\mu\lambda wAv^{tr}\leq (1-\mu^2)vAv^{tr}-\lambda^2wAw^{tr}=\lambda^2(\underbrace{vAv^{tr}-wAw^{tr}}_{\geq 0})\) und somit \(2\mu wAv^{tr}\leq \lambda(\underbrace{vAv^{tr}-wAw^{tr}}_{\geq 0})\).\\
Angenommen, \(<w,Av^{tr}=wAv^{tr}\neq 0\). O.E. sei dann \(wAv^{tr}>0\) (sonst ersetze \(w\) durch \(-w\) \(\lightning\)).\\
Dann gilt: \(0<\underbrace{2\mu}_{\rightarrow 2}(\underbrace{wAv^{tr}}_{>0})=\leq \underbrace{\lambda}_{\rightarrow 0} (\underbrace{vAv^{tr}-wAw^{tr}}_{>0})\). Nun verwende \(lim_{\lambda\rightarrow 0}\) \(\lightning\).
\(\Rightarrow \) \ul{Zw.Beh.}\\
Betrachte nun \(U=(\mathbb{R}\cdot v)^\bot\). Es gilt \(U^\bot=\mathbb{R}\cdot v\) und für alle \(w\in U\) gilt \(w\bot Av^{tr}\), also \(Av^{tr}\bot U\). Dies liefert \(Av^{tr}\bot U\). Dies liefert \(Av^{tr}\in U^{bot}=\mathbb{R}v\). Somit existiert ein \(c\in\mathbb{R}\) mit \(Av^{tr}=cv^{tr}\). Somit ist \(c\) ein Eigenwert von \(A\).
\qed
\subsubsection{Korollar}
Ist \(V\) ein \(n\)-dimensionaler Vektorraum und \(\phi\in ENd_\mathbb{R}(V)\), so dass eine Basis \(B\) von \(V\) existiert, für die \(M_B(\phi)\) symmetrisch ist, so hat \(\phi\) einen reellen Eigenwert.
\subsubsection{Definition}
Sei \((V,\Phi)\) ein euklidischer Vektorraum und \(\phi\in End_\mathbb{R}(V)\). Die Abbildung \(\phi\) heißt \ul{selbstadjungiert}, wenn für \(v,w\in V\) gilt: \(\Phi(\phi(v),w)=\Phi(v,\phi(w))\).
\subsubsection{Satz (Charakterisierung selbstadjungierter Endomorphismen)}
Sei \((V,\Phi)\) ein euklidischer Vektorraum und \(\phi\in End_\mathbb{R}(V)\). Dann sind die folgenden Bedingungen äquivalent:\\
(a) \(\phi\) ist selbstadjungiert
(b) Ist \(B=(v_1,\dots,v_n)\) eine ONB von \(V\), so gilt \(\Phi(\phi(v_i), v_j)=\Phi(v_i,\phi(v_j))\) für \(^\leq i,j\leq n\).
(c) Für jede ONB \(B\) von \(V\) ist \(M_B(\phi)\) eine symmetrische Matrix
(d) Es gibt eine ONB \(B\) von \(V\), so dass \(M_B(\phi)\) eine symmetrische Matrix ist.\\
\proof
"(a)\(\Rightarrow\)(b)" klar.\\
"(b)\(\Rightarrow\)(c)" Sei \(B=(v_1,\dots,v_n)\) und \(M_B(\phi)=(a_ij)\). Dann gilt \(\phi(v_j)=a_{1j}v_1+\dots+a_{nj}v_n\) für \(j=1,\dots,n\).\\
Es folgt: \(\Phi(\phi(v_i), v_j)=\Phi(a_{1i}v_1+\dots+a_{ni}v_n,v_j)=a_{ji}\Phi(v_j,v_j)=a_{ji}\) und \(\Phi(v_i,\phi(v_j))=\Phi(v_i,a_{1j}v_1+\dots+a_{nj}v_n)=a_{ij}\Phi(v_i,v_i)=a_{ij}\). Für \(i,j=1,\dots,n\) gilt also \(a_{ji}=a_{ij}\), d.h. \(M_B(\phi)\) ist symmetrisch.\\
"(c)\(\Rightarrow\)(d)" klar\\
"(d)\(\Rightarrow\)(a)" Sei \(M_B(\phi)=(a_{ij})\). Für \(v=b_1v_1+\dots+b_nv_n\) und \(w=c_1v_1+\dots+c_nv_n\) mit \(b_i,c_j\in\mathbb{R}\) gilt, dann \(\Phi(\phi(v),w)=\sum_{i=1}^n\sum_{j=1}^nb_ic_j\cdot \Phi(\phi(v_i),v_j)=\sum_{i=1}^n\sum_{j=1}^nc_ic_ja_{ji}\overset{M_B(\phi)\text{ symm.}}{=}\sum_{i=1}^n\sum_{j=1}^nb_ic_j\cdot \Phi(v_i,\phi(v_j))=\Phi(v,\phi(w))\).
\qed
\subsubsection{Theorem (Diagonalisierbarkeit selbstadjungierter Endomorphismen)}
Sei \((V,\Phi)\) ein euklidischer Vektorraum und \(\phi:V\rightarrow V\) ein selbstadjungierter Endomorphismus.\\
Dann gibt es eine ONB \(B\) von \(V\), so dass \(M_B(\phi)\) eine Diagonalmatrix ist.\\
\proof
Wir schließen mit vollständiger Induktion nach \(n=dim_\mathbb{R}(V)\).\\
\ul{\(n=1\)}: klar\\
\ul{\(n>1\)}: Sei \(C\) eine ONB von \(V\). Nach Satz 29.4 ist \(M_C(\phi)\) dann symmetrisch. Nach Theorem 29.1 besitzt \(M_C(\phi)\) einen reellen EW \(\lambda\).\\
Sei \(v\neq 0\) ein EV zum EW \(\lambda\) und \(U=(\mathbb{R}v)^\bot\). Dann gilt \(V=U\obot \mathbb{R}v\) und \(dim_\mathbb{R}(U)=n-1\).\\
Für \(u\in U\) gilt \(\Phi(u,v)=0\) und \(\Phi(\phi(u), v)=\Phi(u,\phi(v))=\Phi(u,\lambda v)=\lambda\Phi(u,v)=0\). Das zeigt \(\phi(u)\in U=(\mathbb{R}v)^\bot\) und somit ist \(U\) \(\phi\)-invariant. Also existiert \(\phi|U:U\rightarrow U\). Offensichtlich ist \(\phi|U\) wieder selbstadjungiert.\\
Somit ist die IV anwendbar und es existiert eine Basis \(\tilde{B}\) von \(U\) so, dass \(M_{\tilde{B}}(\phi|U)\) eine Diagonalmatrix ist. Dann ist \(B=\tilde{B}\cup\{\frac{v}{\norm{V}}\}\) eine ONB von \(V\) und es gilt \(M_B(\phi)=TODO MATRIX\).
\qed
\subsubsection{Korollar (Algorithmus zur Berechnung einer ONB aus EV von \(\phi\))}
Sei \(\phi\in End_\mathbb{R}(V)\) selbstadjungiert. Die folgenden Instruktionen definieren einen Algorithmus, der eine ONB von \(V\) bestehend aus EV von \(\phi\) berechnet.\\
(1) Faktorisiere \(\chi_\phi(x)=(-1)^n(x-\lambda_1)^{\alpha_1}\cdot\dots\cdot (x-\lambda_m)^{\alpha_m}\) mit paarweise verschiedenen \(\lambda_1,\dots,\lambda_m\in\mathbb{R}\) und \(\alpha_i>0\).\\
(2) Für \(i=1,\dots,n\) berechne eine ONB \(B\) von \(Eig(\phi,\lambda_i)\).\\
(3) Gib \(B=B_1\cup\dots\cup B_m\) aus und stoppe.
\subsubsection{Beispiel}
Gegeben sei die \(\mathbb{R}\)-lineare Abbildung \(\phi:\mathbb{R}^3\rightarrow\mathbb{R}^3\) mit \(M_\xi(\phi)=TODO MATRIX=: A\). Offenbar ist \(A\) symmetrisch.\\
Nun wenden wir den Algorithmus aus dem Korollar 29.6 an.\\
(1) \(\chi_\phi(x)=-x^3-x^2+x+1=(-x^2+1)(x+1)=-(x-1)(x+1)^2\)\\
(2) \(Eig(\phi,1)=Ker TODO MATRIX=\mathbb{R}\cdot (5,1,2)\)\\
\(Eig(\phi,-1)=<(0,2,-1),(1,-1,-2)\)\\
Wir orthonormalisieren beide Basen und erhalten \(B_1=\{(\frac{5}{\sqrt{30}},\frac{1}{\sqrt{30}},\frac{2}{\sqrt{30}})\}\) und \(B_2=\{(0,\frac{2}{\sqrt{5}},\frac{-1}{\sqrt{5}}),(\frac{1}{\sqrt{6}},\frac{-1}{\sqrt{6}},\frac{-2}{\sqrt{6}})\}\).\\
(3) Also ist \(B=B_1\cup B_2\) eine ONB von \(\mathbb{R}^3\) und \(M_B(\phi)=\begin{pmatrix}
1 & 0 & 0\\
0 & -1 & 0\\
0 & 0 & -1
\end{pmatrix}\) (Drehung um 180° um die x-Achse)
\end{document}





























